{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| default_exp transform\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import *\n",
    "from networkx import DiGraph\n",
    "from copy import deepcopy\n",
    "\n",
    "from graph_rewrite.core import NodeName, EdgeName, _create_graph, _plot_graph, _graphs_equal, GraphRewriteException\n",
    "from graph_rewrite.lhs import lhs_to_graph\n",
    "from graph_rewrite.match_class import Match, mapping_to_match\n",
    "from graph_rewrite.matcher import find_matches, FilterFunc\n",
    "from graph_rewrite.p_rhs_parse import RenderFunc, p_to_graph, rhs_to_graph\n",
    "from graph_rewrite.rules import Rule, MergePolicy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "_exception_msgs = {\n",
    "    \"no_such_node\": lambda node: f\"Node {node} does not exist in the input graph.\",\n",
    "    \"no_such_edge\": lambda edge: f\"Edge {edge} does not exist in the input graph.\",\n",
    "    \"no_such_attr_in_node\": lambda attr, node: f\"Attribute {attr} does not exist in input graph's node {node}.\",\n",
    "    \"no_such_attr_in_edge\": lambda attr, edge: f\"Attribute {attr} does not exist in input graph's edge {edge}.\",\n",
    "    \"edge_exists\": lambda edge: f\"Edge {edge} already exists in the input graph.\",\n",
    "    \"not_enough_to_merge\": lambda: f\"Tried to merge less than one nodes.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _generate_new_node_name(graph: DiGraph, base_name: NodeName) -> NodeName:\n",
    "    new_name = base_name\n",
    "    i = 0\n",
    "    while new_name in graph.nodes():\n",
    "        i += 1\n",
    "        new_name = f\"{base_name}_{i}\"\n",
    "    return new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _clone_node(graph: DiGraph, node_to_clone: NodeName) -> NodeName:\n",
    "    if node_to_clone not in graph.nodes:\n",
    "        raise GraphRewriteException(_exception_msgs[\"no_such_node\"](node_to_clone))\n",
    "\n",
    "    # Create a new node name\n",
    "    clone_name = _generate_new_node_name(graph, node_to_clone)\n",
    "\n",
    "    # Add this new node to graph\n",
    "    cloned_node_attrs = graph.nodes(data=True)[node_to_clone]\n",
    "    graph.add_node(clone_name, **cloned_node_attrs)\n",
    "\n",
    "    # Clone edges (connect the clone to all original edge endpoints + copy attrs)\n",
    "    for n, _ in graph.in_edges(node_to_clone):\n",
    "        if (n, clone_name) not in graph.edges():\n",
    "            cloned_edge_attrs = graph.edges[n, node_to_clone]\n",
    "            graph.add_edge(n, clone_name, **cloned_edge_attrs)\n",
    "    \n",
    "    for _, n in graph.out_edges(node_to_clone):\n",
    "        if (clone_name, n) not in graph.edges():\n",
    "            cloned_edge_attrs = graph.edges[node_to_clone, n]\n",
    "            graph.add_edge(clone_name, n, **cloned_edge_attrs)\n",
    " \n",
    "    return clone_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _remove_node(graph: DiGraph, node_to_remove: NodeName):\n",
    "    if node_to_remove not in graph.nodes():\n",
    "        raise GraphRewriteException(_exception_msgs[\"no_such_node\"](node_to_remove))\n",
    "    graph.remove_node(node_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _remove_edge(graph: DiGraph, edge_to_remove: EdgeName):\n",
    "    if edge_to_remove not in graph.edges():\n",
    "        raise GraphRewriteException(_exception_msgs[\"no_such_edge\"](edge_to_remove))\n",
    "    graph.remove_edge(*edge_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _remove_node_attrs(graph: DiGraph, node: NodeName, attrs_to_remove: set):\n",
    "    if node not in graph.nodes():\n",
    "        raise GraphRewriteException(_exception_msgs[\"no_such_node\"](node))\n",
    "    for attr in attrs_to_remove:\n",
    "        if attr not in graph.nodes[node]:\n",
    "            raise GraphRewriteException(_exception_msgs[\"no_such_attr_in_node\"](attr, node))\n",
    "        del graph.nodes[node][attr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _remove_edge_attrs(graph: DiGraph, edge: EdgeName, attrs_to_remove: set):\n",
    "    if edge not in graph.edges():\n",
    "        raise GraphRewriteException(_exception_msgs[\"no_such_edge\"](edge))\n",
    "    for attr in attrs_to_remove:\n",
    "        if attr not in graph.edges[edge]:\n",
    "            raise GraphRewriteException(_exception_msgs[\"no_such_attr_in_edge\"](attr, edge))\n",
    "        del graph.edges[edge][attr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _merge_nodes(graph: DiGraph, nodes_to_merge: set[NodeName], merge_policy) -> NodeName:\n",
    "    if len(nodes_to_merge) < 1:\n",
    "        raise GraphRewriteException(_exception_msgs[\"not_enough_to_merge\"]())\n",
    "    elif len(nodes_to_merge) == 1:\n",
    "        return list(nodes_to_merge)[0]\n",
    "    for node_to_merge in nodes_to_merge:\n",
    "        if node_to_merge not in graph.nodes:\n",
    "            raise GraphRewriteException(_exception_msgs[\"no_such_node\"](node_to_merge))\n",
    "\n",
    "    merged_node_name = _generate_new_node_name(graph, \"&\".join(nodes_to_merge))\n",
    "    \n",
    "    merged_node_attrs = {}\n",
    "    merged_src_nodes, merged_target_nodes = set(), set()\n",
    "    merged_src_attrs, merged_target_attrs = {}, {} # map a src/target node to the edge's merged attrs\n",
    "    self_loop, self_loop_attrs = False, {}\n",
    "    \n",
    "    for node_to_merge in nodes_to_merge:\n",
    "        merged_node_attrs = merge_policy(merged_node_attrs, graph.nodes[node_to_merge])\n",
    "        \n",
    "        in_edges, out_edges = graph.in_edges(node_to_merge), graph.out_edges(node_to_merge)\n",
    "        merged_src_nodes.update({s if s not in nodes_to_merge else merged_node_name for s, _ in in_edges})\n",
    "        merged_target_nodes.update({t if t not in nodes_to_merge else merged_node_name for _, t in out_edges})\n",
    "\n",
    "        for edge in in_edges:\n",
    "            edge_attrs, src = graph.edges[edge], edge[0]\n",
    "            # Add to source attributes\n",
    "            if src not in merged_src_attrs.keys():\n",
    "                merged_src_attrs[src] = edge_attrs\n",
    "            else:\n",
    "                merged_src_attrs[src] = merge_policy(merged_src_attrs[src], edge_attrs)\n",
    "\n",
    "            # Handle selp loop\n",
    "            if src in nodes_to_merge:\n",
    "                self_loop = True\n",
    "                self_loop_attrs = merge_policy(self_loop_attrs, edge_attrs)\n",
    "            \n",
    "        for edge in out_edges:\n",
    "            edge_attrs, target = graph.edges[edge], edge[1]\n",
    "            # Add to source attributes\n",
    "            if target not in merged_target_attrs.keys():\n",
    "                merged_target_attrs[target] = edge_attrs\n",
    "            else:\n",
    "                merged_target_attrs[target] = merge_policy(merged_target_attrs[target], edge_attrs)\n",
    "\n",
    "            # Handle selp loop\n",
    "            if target in nodes_to_merge:\n",
    "                self_loop = True\n",
    "                self_loop_attrs = merge_policy(self_loop_attrs, edge_attrs)\n",
    "\n",
    "        graph.remove_node(node_to_merge)\n",
    "\n",
    "    # Add merged node to graph\n",
    "    graph.add_node(merged_node_name, **merged_node_attrs)\n",
    "\n",
    "    # Add merged source and target edges (including a new self loop)\n",
    "    if self_loop:\n",
    "        graph.add_edge(merged_node_name, merged_node_name, **self_loop_attrs)\n",
    "    for src_node in merged_src_nodes:\n",
    "        if (src_node, merged_node_name) not in graph.edges():\n",
    "            graph.add_edge(src_node, merged_node_name)\n",
    "    for target_node in merged_src_nodes:\n",
    "        if (merged_node_name, target_node) not in graph.edges():\n",
    "            graph.add_edge(merged_node_name, target_node)\n",
    "    \n",
    "    # Add edge attributes (other than selp loop)\n",
    "    for src_node, attrs in merged_src_attrs.items():\n",
    "        if src_node not in nodes_to_merge:\n",
    "            graph.update(edges=[(src_node, merged_node_name, merged_src_attrs[src_node])])\n",
    "            # graph.edges[(src_node, merged_node_name)] = merged_src_attrs[src_node]\n",
    "    for target_node, attrs in merged_target_attrs.items():\n",
    "        if target_node not in nodes_to_merge:\n",
    "            graph.update(edges=[(merged_node_name, target_node, merged_target_attrs[target_node])])\n",
    "            # graph.edges[(merged_node_name, target_node)] = merged_target_attrs[target_node]\n",
    "\n",
    "    return merged_node_name       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _add_node(graph: DiGraph, node_to_add: NodeName) -> NodeName:\n",
    "    # Create a new node name\n",
    "    new_name = _generate_new_node_name(graph, node_to_add)\n",
    "    graph.add_node(new_name)\n",
    "    return new_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _add_edge(graph: DiGraph, edge_to_add: EdgeName):\n",
    "    src, target = edge_to_add\n",
    "    if src not in graph.nodes():\n",
    "        raise GraphRewriteException(_exception_msgs[\"no_such_node\"](src))\n",
    "    elif target not in graph.nodes():\n",
    "        raise GraphRewriteException(_exception_msgs[\"no_such_node\"](target))\n",
    "    elif edge_to_add in graph.edges():\n",
    "        raise GraphRewriteException(_exception_msgs[\"edge_exists\"](edge_to_add))\n",
    "    else:\n",
    "        graph.add_edge(src, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _add_node_attrs(graph: DiGraph, node: NodeName, attrs_to_add: dict):\n",
    "    if node not in graph.nodes():\n",
    "        raise GraphRewriteException(_exception_msgs[\"no_such_node\"](node))\n",
    "    for attr, val in attrs_to_add.items():\n",
    "        graph.nodes[node][attr] = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _add_edge_attrs(graph: DiGraph, edge: EdgeName, attrs_to_add: dict):\n",
    "    if edge not in graph.edges():\n",
    "        raise GraphRewriteException(_exception_msgs[\"no_such_edge\"](edge))\n",
    "    for attr, val in attrs_to_add.items():\n",
    "        graph.edges[edge][attr] = val"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "_GREEN = '\\033[92m'\n",
    "_RED = '\\033[91m'\n",
    "_BLACK = '\\033[0m'\n",
    "\n",
    "def _log(msg, is_log: bool, color=_BLACK):\n",
    "    if is_log:\n",
    "        print(f\"{color}{msg}{_BLACK}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _rewrite_match_restrictive(input_graph: DiGraph, rule: Rule, lhs_input_map: dict[NodeName, NodeName], is_log: bool) -> dict[NodeName, NodeName]:\n",
    "    # Initialize an empty mapping from P nodes to input_graph nodes.\n",
    "    p_input_map = {}\n",
    "\n",
    "    \"\"\"Clone nodes:\n",
    "        Find all LHS nodes that should be cloned (and what are their clones in P).\n",
    "        For each clone of an LHS node (apart from the first one), add it to the input graph \n",
    "        (clone with edges and attributes) and add the pair (clone_name, lhs_node_name) to the p->input mapping.\n",
    "    \"\"\"\n",
    "    # Map each cloned lhs node to a flag, denoting whether the original node is reused with the same name in P.\n",
    "    # If not, the original node will be removed later\n",
    "    cloned_to_flags_map = {cloned_lhs_node: False for cloned_lhs_node in rule.nodes_to_clone().keys()}\n",
    "    for cloned_lhs_node, p_clones in rule.nodes_to_clone().items():\n",
    "        for p_clone in p_clones:\n",
    "            # Original cloned node is reused in P, preserve it\n",
    "            if p_clone == cloned_lhs_node:\n",
    "                _log(f\"Clone {lhs_input_map[cloned_lhs_node]}\", is_log)\n",
    "                cloned_to_flags_map[cloned_lhs_node] = True\n",
    "                p_input_map[p_clone] = lhs_input_map[cloned_lhs_node]\n",
    "            # All other clones require actual cloning (mapped to the new cloned node in input graph)\n",
    "            else:\n",
    "                new_clone_id = _clone_node(input_graph, lhs_input_map[cloned_lhs_node])\n",
    "                _log(f\"Clone {lhs_input_map[cloned_lhs_node]} as {new_clone_id}\", is_log)\n",
    "                p_input_map[p_clone] = new_clone_id\n",
    "\n",
    "    \"\"\"Remove nodes, complete p->input mapping with preserved nodes which are not clones:\n",
    "        Find all LHS nodes that should be removed. \n",
    "        For each LHS node, if should be removed - remove it from input.\n",
    "                            otherwise, if it is not a clone, add to the mapping.\n",
    "    \"\"\"\n",
    "    for lhs_node in rule.lhs.nodes():\n",
    "        # Cloned lhs nodes which weren't reused and so, should be deleted\n",
    "        if lhs_node in rule.nodes_to_remove() or (lhs_node in cloned_to_flags_map.keys() and cloned_to_flags_map[lhs_node] == False):\n",
    "            _log(f\"Remove node {lhs_input_map[lhs_node]}\", is_log)\n",
    "            _remove_node(input_graph, lhs_input_map[lhs_node])        \n",
    "        # Else, either a saved cloned node (already preserved) or a regular one (should preserve them)\n",
    "        elif lhs_node not in cloned_to_flags_map.keys():\n",
    "            p_node = list(rule._rev_p_lhs[lhs_node])[0]\n",
    "            p_input_map[p_node] = lhs_input_map[lhs_node]\n",
    "\n",
    "    # Remove edges.\n",
    "    for lhs_src, lhs_target in rule.edges_to_remove():\n",
    "        _log(f\"Remove edge ({p_input_map[lhs_src]}, {p_input_map[lhs_target]})\", is_log)\n",
    "        _remove_edge(input_graph, (p_input_map[lhs_src], p_input_map[lhs_target]))\n",
    "\n",
    "    # Remove node attrs.\n",
    "    for p_node, attrs_to_remove in rule.node_attrs_to_remove().items():\n",
    "        _log(f\"Remove attrs {attrs_to_remove} from node {p_input_map[p_node]}\", is_log)\n",
    "        _remove_node_attrs(input_graph, p_input_map[p_node], attrs_to_remove)\n",
    "\n",
    "    # Remove edge attrs.\n",
    "    for (p_src, p_target), attrs_to_remove in rule.edge_attrs_to_remove().items():\n",
    "        _log(f\"Remove attrs {attrs_to_remove} from edge {(p_input_map[p_src], p_input_map[p_target])}\", is_log)\n",
    "        _remove_edge_attrs(input_graph, (p_input_map[p_src], p_input_map[p_target]), attrs_to_remove)\n",
    "\n",
    "    return p_input_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _rewrite_match_expansive(input_graph: DiGraph, rule: Rule, p_input_map: dict[NodeName, NodeName], is_log: bool):\n",
    "    # Initialize an empty mapping from RHS nodes to input_graph nodes.\n",
    "    rhs_input_map = {}\n",
    "\n",
    "    \"\"\"Merge nodes:\n",
    "        Find all RHS nodes that are a merge of P nodes (and what P nodes they merge).\n",
    "        For each RHS node that is a merge, merge all of the relavant P node, add the resulting RHS node to the input.\n",
    "        Also removes from input the P nodes that were merged into a new node.\n",
    "        Save the new RHS merged node to the RHS->input mapping.\n",
    "    \"\"\"\n",
    "    merge_rhs_nodes = rule.nodes_to_merge().keys()\n",
    "    for merge_rhs_node, p_merged in rule.nodes_to_merge().items():\n",
    "        input_nodes_to_merge = {p_input_map[p_node] for p_node in p_merged}\n",
    "        new_merged_id = _merge_nodes(input_graph, input_nodes_to_merge, rule.merge_policy)\n",
    "        _log(f\"Merge {input_nodes_to_merge} as {new_merged_id}\", is_log)\n",
    "        rhs_input_map[merge_rhs_node] = new_merged_id\n",
    "        \n",
    "    \"\"\"Add nodes, complete RHS->input mapping with added (and preserved) nodes:\n",
    "        Find all RHS nodes that should be added to the input (not including nodes that are a merge of P noeds).\n",
    "        For each RHS node, if should be added - add it to input and to the RHS->input mapping.\n",
    "                            otherwise, if is not a merge - add to the mapping.\n",
    "    \"\"\"\n",
    "    for rhs_node in rule.rhs.nodes():\n",
    "        if rhs_node in rule.nodes_to_add():\n",
    "            added_id = _add_node(input_graph, rhs_node)\n",
    "            _log(f\"Add node {rhs_node} as {added_id}\", is_log)\n",
    "            rhs_input_map[rhs_node] = added_id\n",
    "        elif rhs_node not in merge_rhs_nodes:\n",
    "            p_node = list(rule._rev_p_rhs[rhs_node])[0]\n",
    "            rhs_input_map[rhs_node] = p_input_map[p_node]\n",
    "\n",
    "    # Add edges.\n",
    "    for rhs_src, rhs_target in rule.edges_to_add():\n",
    "        _log(f\"Add edge ({rhs_input_map[rhs_src]}, {rhs_input_map[rhs_target]})\", is_log)\n",
    "        _add_edge(input_graph, (rhs_input_map[rhs_src], rhs_input_map[rhs_target]))\n",
    "\n",
    "    # Add node attrs.\n",
    "    for rhs_node, attrs_to_add in rule.node_attrs_to_add().items():\n",
    "        _log(f\"Added attrs {attrs_to_add} to node {rhs_input_map[rhs_node]}\", is_log)\n",
    "        _add_node_attrs(input_graph, rhs_input_map[rhs_node], attrs_to_add)\n",
    "\n",
    "    # Add edge attrs.\n",
    "    for (rhs_src, rhs_target), attrs_to_add in rule.edge_attrs_to_add().items():\n",
    "        _log(f\"Added attrs {attrs_to_add} to edge {(rhs_input_map[rhs_src], rhs_input_map[rhs_target])}\", is_log)\n",
    "        _add_edge_attrs(input_graph, (rhs_input_map[rhs_src], rhs_input_map[rhs_target]), attrs_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _copy_graph(graph: DiGraph) -> DiGraph:\n",
    "    copied_nodes = [(node, deepcopy(attrs)) for node, attrs in graph.nodes(data=True)]\n",
    "    copied_edges = [(s, t, deepcopy(attrs)) for s, t, attrs in graph.edges(data=True)]\n",
    "    copy_graph = DiGraph()\n",
    "    copy_graph.update(nodes=copied_nodes, edges=copied_edges)\n",
    "    return copy_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _restore_graph(graph: DiGraph, last_copy_graph: DiGraph):\n",
    "    graph.clear()\n",
    "    graph.update(nodes=last_copy_graph.nodes(data=True),\n",
    "                 edges=last_copy_graph.edges(data=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _rewrite_match(input_graph: DiGraph, match: Match,\n",
    "                   lhs_graph: DiGraph, p_graph: DiGraph, rhs: str,\n",
    "                   render_rhs: dict[str, RenderFunc],\n",
    "                   merge_policy,\n",
    "                   is_log: bool):\n",
    "\n",
    "    _log(f\"Transform match: {match.mapping}\", is_log, _GREEN)\n",
    "\n",
    "    # Save graph in current state for restoring, if needed\n",
    "    saved_graph = _copy_graph(input_graph)\n",
    "\n",
    "    try:\n",
    "        # Parse RHS according to current match (with render dictionary)\n",
    "        rhs_graph = rhs_to_graph(rhs, match, render_rhs) if rhs else None\n",
    "        rule = Rule(lhs_graph, p_graph, rhs_graph, merge_policy=merge_policy)\n",
    "        # Transform the graph\n",
    "        lhs_input_map = match.mapping\n",
    "        p_input_map = _rewrite_match_restrictive(input_graph, rule, lhs_input_map, is_log)\n",
    "        _rewrite_match_expansive(input_graph, rule, p_input_map, is_log)\n",
    "        _log(f\"Nodes: {input_graph.nodes(data=True)}\\nEdges: {input_graph.edges(data=True)}\\n\", is_log, _GREEN)\n",
    "        return match\n",
    "\n",
    "    except GraphRewriteException as e:\n",
    "        _log(f\"Failed to transform: {e.message}\", is_log, _RED)\n",
    "        _restore_graph(input_graph, saved_graph)\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite Graph\n",
    "Our library's main function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rewrite(input_graph: DiGraph, lhs: str, p: str = None, rhs: str = None,\n",
    "                   condition: FilterFunc = lambda match: True,\n",
    "                   render_rhs: dict[str, RenderFunc] = {},\n",
    "                   merge_policy = MergePolicy.choose_last,\n",
    "                   is_log: bool = False,\n",
    "                   is_recursive: bool = False,\n",
    "                   is_lazy: bool = False) -> List[Match]:\n",
    "        \n",
    "        _log(f\"Nodes: {input_graph.nodes(data=True)}\\nEdges: {input_graph.edges(data=True)}\\n\", is_log, _GREEN)\n",
    "\n",
    "        # Parse LHS and P (global for all matches)\n",
    "        lhs_graph, condition = lhs_to_graph(lhs, condition)\n",
    "        p_graph = p_to_graph(p) if p else None\n",
    "        if not is_lazy:\n",
    "            matches = []\n",
    "        \n",
    "        if is_recursive:\n",
    "            next_match = next(find_matches(input_graph, lhs_graph, condition=condition))\n",
    "            while next_match:\n",
    "                new_res = _rewrite_match(input_graph, next_match, lhs_graph, p_graph, rhs, render_rhs, merge_policy, is_log)\n",
    "                if is_lazy:\n",
    "                    yield new_res\n",
    "                else:\n",
    "                    matches.append(new_res)\n",
    "                next_match = next(find_matches(input_graph, lhs_graph, condition=condition))\n",
    "\n",
    "            _log(\"No more matches.\", is_log, _GREEN)\n",
    "            if not is_lazy:\n",
    "                return matches\n",
    "\n",
    "        else:\n",
    "            # Create a duplication of the graph to find matches lazily (actual graph changes between matches)\n",
    "            copy_input_graph = _copy_graph(input_graph)\n",
    "\n",
    "            # Find matches lazily and transform\n",
    "            for match in find_matches(copy_input_graph, lhs_graph, condition=condition):\n",
    "                new_res = _rewrite_match(input_graph, match, lhs_graph, p_graph, rhs, render_rhs, merge_policy, is_log)\n",
    "                if is_lazy:\n",
    "                    yield new_res\n",
    "                else:\n",
    "                    matches.append(new_res)\n",
    "            if not is_lazy:\n",
    "                return matches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating Spanner Workbench Micro-Passes\n",
    "The following cells rewrite the [AST micro-passes presented in the Spanner Workbench project](https://github.com/DeanLight/spanner_workbench/blob/master/src/rgxlog-interpreter/src/rgxlog/grammar/grammar.lark). \n",
    "\n",
    "We assume that the AST graph used in that project is a NetworkX graph (as our library requires). Each node in the graph has a *type* attribute which contains a string with the token-name that node has in the grammer, as well as all the attributes it had in the original, lark-produced AST.\n",
    "\n",
    "In addition, each node has a *idx* attribute which denotes its position among the other children of its parent. That is, if a grammer rule defines ```PAR -> A B C```, then node A will have *idx* value of 0, B will have 1, etc. We use this attribute to handle the fact that in NetworkX graphs, children of some node are unordered.\n",
    "\n",
    "Nodes which denote a constant (int, string, etc.) are constructed as regular nodes with a *val* attribute, containing that constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you'll see in passes such as \"Check Reserved Relation Names\", our library allows a general method to do **assertions and type-checks**. Assume that some check validates that for each pattern LHS in the graph, a condition ```cond(...)``` holds. We will implement it as follows:\n",
    "1. Match the pattern in the graph with LHS.\n",
    "2. Use the ```condition``` parameter to filter the matches according to **the negation of ```cond```**. Thus, only matches for which the condition does **not** hold will remain.\n",
    "3. Loop through the lists of remaining matches, and raise an exception for each one, as it fails the assertion. If none exist, then all matches passed the assertion, the check succeeded and no exception is raised.\n",
    "\n",
    "Since we'll have a lot of checks here, where their specific implementation has nothing to do with our library, we will use a dummy assertion function that receives any number of parameters to visualize how such a check will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _assertion(*args) -> bool:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we will calculate new values based on different parameters and Spanner Workbench classes (which we do not have in this module). Therefore, here's a dummy calculation function that receives any number of parameters and returns some calculated value (we'll use it only when the calculations depends on classes or functions of Spanner which we have no access to):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculation(*args) -> any:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_tokens(ast):\n",
    "    \"\"\"\n",
    "    a lark pass that should be used before the semantic checks\n",
    "    transforms the lark tree by removing the redundant tokens\n",
    "    note that we inherit from 'Transformer' in order to be able to visit token nodes.\n",
    "    \"\"\"\n",
    "    rewrite(ast, lhs='a[type=\"INT\", args]', p='a', rhs='a[val={{val}}]',\n",
    "            render_rhs={'val': lambda match: int(match['a']['args'])})\n",
    "\n",
    "    rewrite(ast, lhs='a[type=\"LOWER_CASE_NAME\", args]', p='a', rhs='a[val={{val}}]',\n",
    "            render_rhs={'val': lambda match: match['a']['args'].value})\n",
    "\n",
    "    rewrite(ast, lhs='a[type=\"UPPER_CASE_NAME\", args]', p='a', rhs='a[val={{val}}]',\n",
    "            render_rhs={'val': lambda match: match['a']['args'].value})\n",
    "\n",
    "    rewrite(ast, lhs='a[type=\"STRING\", args]', p='a', rhs='a[val={{val}}]',\n",
    "            render_rhs={'val': lambda match: match['a']['args'][1:-1]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Reserved Relation Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_reserved_relation_names(ast):\n",
    "    \"\"\"\n",
    "    A lark tree semantic check.\n",
    "    Checks if there are relations in the program with a name that starts with `RESERVED_RELATION_PREFIX`\n",
    "    if such relations exist, throw an exception as this is a reserved name for rgxlog.\n",
    "    \"\"\"\n",
    "    matches = rewrite(ast, lhs='a[type=\"relation_name\"]->rel_name[val]',\n",
    "                      condition=lambda match: _assertion(match['rel_name']['val']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fix_strings(ast):\n",
    "    \"\"\"\n",
    "    Fixes the strings in the lark tree.\n",
    "    Removes the line overflow escapes from strings.\n",
    "    \"\"\"\n",
    "    rewrite(ast, lhs='a[type=\"string\"]->s[val]', rhs='a[type]->s[val={{fixed_val}}]',\n",
    "            render_rhs={'fixed_val': lambda match: match['s']['val'].replace('\\\\\\n', '')})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Span Nodes To Span Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_span_nodes_to_span_instances(ast):\n",
    "    \"\"\"\n",
    "    Converts each span node in the ast to a span instance.\n",
    "    This means that a span in the tree will be represented by a single value (a \"DataTypes.Span\" instance)\n",
    "    instead of two integer nodes, making it easier to work with (as other data types are also represented by\n",
    "    a single value).\n",
    "    \"\"\"\n",
    "    rewrite(ast, lhs='s[type=\"span\"]->start[idx=0]->start_val[val]; s->end[idx=1]->end_val[val]',\n",
    "            p='s[type]', rhs='s[type]->span_child[val={{val}}]',\n",
    "            render_rhs={'val': lambda match: (match['start_val']['val'], match['end_val']['val'])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Statements To Structured Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pass can be implemented partially using our current syntax. A full implenetation will be achievable only after implementing a depth-recursion method in the library's matcher, along with a predefined syntax that denotes such a pattern (\"-+->\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_statements_to_structured_nodes(ast):\n",
    "    \"\"\"\n",
    "    converts each statement node in the tree to a structured node, making it easier to parse in future passes.\n",
    "    a structured node is a class representation of a node in the abstract syntax tree.\n",
    "    note that after using this pass, non statement nodes will no longer appear in the tree, so passes that\n",
    "    should work on said nodes need to be used before this pass in the passes pipeline (e.g. FixString).\n",
    "    \"\"\"\n",
    "    rewrite(ast, lhs=\"\"\"ass[type=\"assignment\"]->var_name_node[idx=0]; ass->value_node[idx=1];\n",
    "            var_name_node->var_name[val]; value_node->value[val]\"\"\",\n",
    "            p='ass[type]', rhs='ass[type]->ass_node[var_name={{var_name}}], value={{value}}, value_type={{value_type}}]',\n",
    "            render_rhs={'var_name': lambda m: m['var_name']['val'], 'value': lambda m: m['value']['val'],\n",
    "                        'var_name': lambda m: type(m['value']['val'])})\n",
    "\n",
    "    rewrite(ast, lhs=\"\"\"rass[type=\"read_assignment\"]->var_name_node[idx=0]; rass->read_arg_node[idx=1];\n",
    "            var_name_node->var_name[val]; read_arg_node->read_arg[val]\"\"\",\n",
    "            p='rass[type]', rhs='rass[type]->rass_node[var_name={{var_name}}, read_arg={{read_arg}}, read_arg_type={{read_arg_type}}]',\n",
    "            render_rhs={'var_name': lambda m: m['var_name']['val'], 'read_arg': lambda m: m['read_arg']['val'],\n",
    "                        'read_arg_type': lambda m: type(m['read_arg']['val'])})\n",
    "    \n",
    "    # add_fact, remove_fact, query, relation_declaration and rule all use list tokens, that define a recursive list of unknown length.\n",
    "    # While such pattern cannot be identified with our current syntax, it WILL be possible when implementing a depth-recursive,\n",
    "    # a concept which can be assigned with the syntax -+->."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Defined Referenced Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes the existence of a symbol table, in addition to the AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_defined_referenced_variables(ast, sym_tab):\n",
    "    \"\"\"\n",
    "    A lark tree semantic check.\n",
    "    checks whether each variable reference refers to a defined variable.\n",
    "    \"\"\"\n",
    "    rewrite(ast, lhs='_[type=\"assignment\"]->ass_node[value_type, value]',\n",
    "            condition=lambda match: _assertion(match['ass_node']['value_type'], match['ass_node']['value']))\n",
    "\n",
    "    rewrite(ast, lhs='_[type=\"read_assignment\"]->rass_node[read_arg_type, read_arg]',\n",
    "            condition=lambda match: _assertion(match['rass_node']['read_arg_type'], match['rass_node']['read_arg']))\n",
    "\n",
    "    rewrite(ast, lhs='_[type=\"add_fact]->afact_node[term_list, type_list]',\n",
    "            condition=lambda match: _assertion(match['afact_node']['term_list'], match['afact_node']['type_list'], sym_tab))\n",
    "    \n",
    "    rewrite(ast, lhs='_[type=\"remove_fact]->rfact_node[term_list, type_list]',\n",
    "            condition=lambda match: _assertion(match['rfact_node']['term_list'], match['rfact_node']['type_list'], sym_tab))\n",
    "\n",
    "    rewrite(ast, lhs='_[type=\"query\"]->query_node[term_list, type_list]',\n",
    "            condition=lambda match: _assertion(match['query_node']['term_list'], match['query_node']['type_list'], sym_tab))\n",
    "\n",
    "    rewrite(ast, lhs='_[type=\"rule\"]->rule_node[body_relation_list, body_relation_type_list]',\n",
    "            condition=lambda match: _assertion(match['rule_node']['body_relation_list'], match['rule_node']['type_list'], sym_tab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Referenced Relations Existence And Arity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes the existence of a symbol table, in addition to the AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_referenced_relations_existence_and_arity(ast, sym_tab):\n",
    "    \"\"\"\n",
    "    A lark tree semantic check.\n",
    "    Checks whether each normal relation (that is not an ie relation) reference refers to a defined relation.\n",
    "    Also checks if the relation reference uses the correct arity.\n",
    "    \"\"\"\n",
    "    rewrite(ast, lhs='_[type=\"query\"]->query_node[relation_name, term_list]',\n",
    "            condition=lambda match: _assertion(match['query_node']['relation_name'], match['query_node']['term_list'], sym_tab))\n",
    "    \n",
    "    rewrite(ast, lhs='_[type=\"add_fact\"]->afact_node[relation_name, term_list]',\n",
    "            condition=lambda match: _assertion(match['afact_node']['relation_name'], match['afact_node']['term_list'], sym_tab))\n",
    "\n",
    "    rewrite(ast, lhs='_[type=\"remove_fact\"]->rfact_node[relation_name, term_list]',\n",
    "            condition=lambda match: _assertion(match['rfact_node']['relation_name'], match['rfact_node']['term_list'], sym_tab))\n",
    "\n",
    "    rewrite(ast, lhs='_[type=\"rule\"]->rule_node[body_relation_list, body_relation_type_list]',\n",
    "            condition=lambda match: _assertion(match['rule_node']['body_relation_list'], match['rule_node']['type_list'], sym_tab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Referenced IE Relations Existence And Arity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes the existence of a symbol table, in addition to the AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_referenced_ie_relations_existence_and_arity(ast, sym_tab):\n",
    "    \"\"\"\n",
    "    A lark tree semantic check.\n",
    "    Checks whether each ie relation reference refers to a defined ie function.\n",
    "    Also checks if the correct input arity and output arity for the ie function were used.\n",
    "\n",
    "    currently, an ie relation can only be found in a rule's body, so this is the only place where this\n",
    "    check will be performed.\n",
    "    \"\"\"\n",
    "    rewrite(ast, lhs='_[type=\"rule\"]->rule_node[body_relation_list, body_relation_type_list]',\n",
    "            condition=lambda match: _assertion(match['rule_node']['body_relation_list'], match['rule_node']['type_list'], sym_tab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Rule Safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_rule_safety(ast):\n",
    "    \"\"\"\n",
    "    A lark tree semantic check.\n",
    "    checks whether the rules in the programs are safe.\n",
    "    \"\"\"\n",
    "    rewrite(ast, lhs='_[type=\"rule\"]->rule_node[head_relation, body_relation_list, body_relation_type_list]',\n",
    "            condition=lambda match: _assertion(match['rule_node']['head_relation'], match['rule_node']['body_relation_list'], match['rule_node']['type_list']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type Check Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes the existence of a symbol table, in addition to the AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _type_check_assignments(ast, sym_tab):\n",
    "    \"\"\"\n",
    "    a lark semantic check\n",
    "    performs type checking for assignments\n",
    "    in the current version of lark, this type checking is only required for read assignments.\n",
    "    \"\"\"\n",
    "    rewrite(ast, lhs='_[type=\"assignment\"]->ass_node[read_arg_type, read_arg]',\n",
    "            condition=lambda match: _assertion(match['ass_node']['read_arg_type'], match['ass_node']['read_arg'], sym_tab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type Check Relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes the existence of a symbol table, in addition to the AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _type_check_relations(ast, sym_tab):\n",
    "    \"\"\"\n",
    "    A lark tree semantic check.\n",
    "    This pass makes the following assumptions and might not work correctly if they are not met\n",
    "    1. that relations and ie relations references and correct arity were checked.\n",
    "    2. variable references were checked.\n",
    "    3. it only gets a single statement as an input.\n",
    "\n",
    "    this pass performs the following checks:\n",
    "    1. checks if relation references are properly typed.\n",
    "    2. checks if ie relations are properly typed.\n",
    "    3. checks if free variables in rules do have conflicting types.\n",
    "    \"\"\"\n",
    "    rewrite(ast, lhs='_[type=\"add_fact\"]->afact_node[relation_name, term_list, type_list]',\n",
    "            condition=lambda match: _assertion(match['afact_node']['relation_name'], match['afact_node']['term_list'], \n",
    "                                               match['afact_node']['type_list'], sym_tab))\n",
    "    \n",
    "    rewrite(ast, lhs='_[type=\"remove_fact\"]->rfact_node[relation_name, term_list, type_list]',\n",
    "            condition=lambda match: _assertion(match['rfact_node']['relation_name'], match['rfact_node']['term_list'], \n",
    "                                               match['rfact_node']['type_list'], sym_tab))\n",
    "\n",
    "    rewrite(ast, lhs='_[type=\"query\"]->query_node[relation_name, term_list, type_list]',\n",
    "            condition=lambda match: _assertion(match['query_node']['relation_name'], match['query_node']['term_list'], \n",
    "                                               match['query_node']['type_list'], sym_tab))\n",
    "\n",
    "    rewrite(ast, lhs='_[type=\"rule\"]->rule_node[body_relation_list, body_relation_type_list]',\n",
    "            condition=lambda match: _assertion(match['rule_node']['body_relation_list'], match['rule_node']['type_list'], sym_tab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Declared Relations Schemas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes the existence of a symbol table, in addition to the AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_declared_relations_schemas(ast, sym_tab):\n",
    "    \"\"\"\n",
    "    this pass writes the relation schemas that it finds in relation declarations and rule heads* to the\n",
    "    symbol table.\n",
    "    * note that a rule is a relation declaration of the rule head relation and a definition of its contents\n",
    "\n",
    "    this pass assumes that type checking was already performed on its input.\n",
    "    \"\"\"\n",
    "    matches = rewrite(ast, lhs='_[type=\"relation_declaration\"]->reldec_node[relation_name, type_list]')\n",
    "    for match in matches:\n",
    "        # Does some imperative side-effect that adds to the sym_tab, based on the match's relation_name, type_list\n",
    "        pass\n",
    "\n",
    "    matches = rewrite(ast, lhs='_[type=\"rule\"]->rule_node[body_relation_list, body_relation_type_list]',\n",
    "                      condition=lambda match: _assertion(match['rule_node']['body_relation_list'], match['rule_node']['body_relation_type_list'], sym_tab))\n",
    "    for match in matches:\n",
    "        # Does some imperative side-effect that adds each relation in the rule to the sym_tab\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resolve Variables References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes the existence of a symbol table, in addition to the AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resolve_variables_references(ast, sym_tab):\n",
    "    \"\"\"\n",
    "    a lark execution pass\n",
    "    this pass replaces variable references with their literal values.\n",
    "    also replaces DataTypes.var_name types with the real type of the variable.\n",
    "    \"\"\"\n",
    "    rewrite(ast, lhs='ass[type=\"assignment\"]->ass_node[value_type, value]',\n",
    "            condition=lambda match: _assertion(match['ass_node']['value_type']),\n",
    "            rhs='ass[type]->ass_node[value_type={{value_type}}, value={{value}}]',\n",
    "            render_rhs={\n",
    "                'value_type': lambda match: _calculation(match['ass_node']['value'], sym_tab),\n",
    "                'value': lambda match: _calculation(match['ass_node']['value'], sym_tab)})\n",
    "    \n",
    "    rewrite(ast, lhs='rass[type=\"read_assignment\"]->rass_node[read_arg_type, read_arg]',\n",
    "        condition=lambda match: _assertion(match['rass_node']['read_arg_type']),\n",
    "        rhs='ass[type]->rass_node[read_arg_type={{read_arg_type}}, read_arg={{read_arg}}]',\n",
    "        render_rhs={\n",
    "            'read_arg_type': lambda match: _calculation(match['rass_node']['read_arg'], sym_tab),\n",
    "            'read_arg': lambda match: _calculation(match['rass_node']['read_arg'], sym_tab)})\n",
    "\n",
    "    rewrite(ast, lhs='query[type=\"query\"]->query_node[term_list, type_list]',\n",
    "            rhs='query[type]->query_node[term_list={{term_list}}, type_list={{type_list}}]',\n",
    "            render_rhs={\n",
    "                'term_list': lambda match: _calculation(match['query_node']['term_list'], match['query_node']['type_list'], sym_tab),\n",
    "                'type_list': lambda match: _calculation(match['query_node']['term_list'], match['query_node']['type_list'], sym_tab)\n",
    "            })\n",
    "    \n",
    "    rewrite(ast, lhs='afact[type=\"add_fact\"]->afact_node[term_list, type_list]',\n",
    "            rhs='afact[type]->afact_node[term_list={{term_list}}, type_list={{type_list}}]',\n",
    "            render_rhs={\n",
    "                'term_list': lambda match: _calculation(match['afact_node']['term_list'], match['afact_node']['type_list'], sym_tab),\n",
    "                'type_list': lambda match: _calculation(match['afact_node']['term_list'], match['afact_node']['type_list'], sym_tab)\n",
    "            })\n",
    "\n",
    "    rewrite(ast, lhs='rfact[type=\"remove_fact\"]->rfact_node[term_list, type_list]',\n",
    "            rhs='rfact[type]->rfact_node[term_list={{term_list}}, type_list={{type_list}}]',\n",
    "            render_rhs={\n",
    "                'term_list': lambda match: _calculation(match['rfact_node']['term_list'], match['rfact_node']['type_list'], sym_tab),\n",
    "                'type_list': lambda match: _calculation(match['rfact_node']['term_list'], match['rfact_node']['type_list'], sym_tab)\n",
    "            })\n",
    "    \n",
    "    rewrite(ast, lhs='rule[type=\"rule\"]->rule_node[body_relation_list, body_relation_type_list]',\n",
    "            condition=lambda match: _assertion(match['rule_node']['body_relation_list'], match['rule_node']['body_relation_type_list']),\n",
    "            rhs='rule[type=\"rule\"]->rule_node[body_relation_list={{brl}}, body_relation_type_list={{brtl}}]',\n",
    "            render_rhs={\n",
    "                'brl': lambda match: _calculation(match['rule_node']['body_relation_list'], match['rule_node']['body_relation_type_list'], sym_tab),\n",
    "                'brtl': lambda match: _calculation(match['rule_node']['body_relation_list'], match['rule_node']['body_relation_type_list'], sym_tab)\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes the existence of a symbol table, in addition to the AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _execute_assignments(ast, sym_tab):\n",
    "    \"\"\"\n",
    "    a lark execution pass\n",
    "    executes assignments by saving variables' values and types in the symbol table\n",
    "    should be used only after variable references are resolved, meaning the assigned values and read() arguments\n",
    "    are guaranteed to be literals.\n",
    "    \"\"\"\n",
    "    matches = rewrite(ast, lhs='ass[type=\"assignment\"]->ass_node[var_name, value, value_type]')\n",
    "    for match in matches:\n",
    "        # Imperative side-effect that uses each match's var_name, value, value_type and updates the sym_tab (executes assignment)\n",
    "        pass\n",
    "\n",
    "    matches = rewrite(ast, lhs='rass[type=\"read_assignment\"]->rass_node[read_arg, var_name]')\n",
    "    for match in matches:\n",
    "        # Imperative side-effect that uses each match's read_arg, var_name and updates the sym_tab (executes assignment)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add Statements To Netx Parse Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assumes the existence of a parse graph, in addition to the AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _add_statements_to_netx_parse_graph(ast, parse_graph):\n",
    "    \"\"\"\n",
    "    A lark execution pass.\n",
    "    This pass adds each statement in the input parse tree to the parse graph.\n",
    "    This pass is made to work with execution.naive_execution as the execution function and\n",
    "    term_graph.NetxStateGraph as the parse graph.\n",
    "\n",
    "    Each statement in the parse graph will be a child of the parse graph's root.\n",
    "\n",
    "    Each statement in the parse graph will have a type attribute that contains the statement's name in the\n",
    "    rgxlog grammar.\n",
    "\n",
    "    Some nodes in the parse graph will contain a value attribute that would contain a relation that describes\n",
    "    that statement.\n",
    "    e.g. a add_fact node would have a value which is a structured_nodes.AddFact instance\n",
    "    (which inherits from structured_nodes.Relation) that describes the fact that will be added.\n",
    "\n",
    "    Some statements are more complex and will be described by more than a single node, e.g. a rule node.\n",
    "    The reason for this is that we want a single netx node to not contain more than one Relation\n",
    "    (or IERelation) instance. This will make the parse graph a \"graph of relation nodes\", allowing\n",
    "     for flexibility for optimization in the future.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
