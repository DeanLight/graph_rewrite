{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LHS Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This module defines the grammar of the LHS that is given by the user to the *rewrite* function of the library.\n",
    "The module is also responsible for parsing of the pattern sent as LHS, into a networkX graph representing the template to search.\n",
    "\n",
    "The module converts the declerative constraints regarding the properties of the nodes and edges in the LHS, to imperative functions that are checked together with the 'condition' parameter of *rewrite*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lark.lark.Lark"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import copy\n",
    "from collections.abc import Callable\n",
    "import networkx as nx\n",
    "from lark import Transformer, Lark\n",
    "from lark import UnexpectedCharacters, UnexpectedToken\n",
    "from graph_rewrite.match_class import Match\n",
    "from graph_rewrite.core import GraphRewriteException\n",
    "from graph_rewrite.core import _create_graph,  _graphs_equal, draw\n",
    "from collections import defaultdict\n",
    "from graph_rewrite.match_class import _convert_to_edge_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar\n",
    "The grammar induces the allowed syntax of a legal LHS string that can be provided by the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "lhs_parser = Lark(r\"\"\"\n",
    "    %import common.INT -> INT \n",
    "    %import common.FLOAT -> FLOAT\n",
    "    %import common.ESCAPED_STRING -> STRING\n",
    "    %import common.WS -> WS\n",
    "    %ignore WS\n",
    "\n",
    "    NAMED_VERTEX: /[_a-zA-Z0-9]+/\n",
    "    ANONYMUS: \"_\"\n",
    "    ATTR_NAME: /[_a-zA-Z0-9]+/\n",
    "    TYPE:  \"int\" | \"str\" | \"bool\" | \"float\"\n",
    "    BOOLEAN: \"True\" | \"False\"\n",
    "    NATURAL_NUMBER: /[1-9][0-9]*/\n",
    "    INDEX: /[0-9]+/\n",
    "\n",
    "    value: FLOAT | INT | BOOLEAN | STRING\n",
    "\n",
    "    attribute: ATTR_NAME [\":\" TYPE] [\"=\" value]\n",
    "    attributes: \"[\" attribute (\",\" attribute)* \"]\"\n",
    "\n",
    "    multi_connection: \"-\" NATURAL_NUMBER [attributes] \"->\" \n",
    "    connection: [\"-\" attributes]\"->\"\n",
    "              | multi_connection\n",
    "    \n",
    "    index_vertex: NAMED_VERTEX \"<\" INDEX (\",\" INDEX)* \">\"\n",
    "\n",
    "    vertex: NAMED_VERTEX [attributes]\n",
    "    | index_vertex [attributes]\n",
    "    | ANONYMUS [attributes]\n",
    "\n",
    "    pattern: vertex (connection vertex)*\n",
    "    patterns: pattern (\",\" pattern)* \n",
    "\n",
    "    \"\"\", parser=\"lalr\", start='patterns' , debug=True)\n",
    "\n",
    "# TODO: Add the \";\" delimiter to the lark grammar - don't think about it on your own, ask Dean\n",
    "\n",
    "# multi_connection: \"-\" NATURAL_NUMBER \"+\" [attributes] \"->\"  - setting for the \"-num+->\" feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "The transformer is designed to return the networkX graph representing the patterns received by the user.\n",
    "\n",
    "For each branch, the appropriate method will be called with the children of the branch as its argument, and its return value will replace the matching node in the tree.\n",
    "\n",
    "The secondary task of the transformer is to collect the node/edge type and constant node/edge value constraints, such that they are added to the 'condition' parameter to be checked later. Thus, the lhsTransformer contains a python dictionary *constraints* which accumulates the constraints from all components of the parsed graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "RenderFunc = Callable[[Match], any] # type of a function to render a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "cnt:int = 0 # unique id for anonymous vertices\n",
    "class graphRewriteTransformer(Transformer):\n",
    "    def __init__(self, visit_tokens: bool = True, component: str = \"LHS\", match: Match = None, render_funcs: dict[str, RenderFunc] = {}) -> None:\n",
    "        super().__init__(visit_tokens)\n",
    "        # general\n",
    "        self.component = component\n",
    "        # RHS parameters\n",
    "        self.match = match\n",
    "        self.render_funcs = render_funcs\n",
    "        # LHS parameters\n",
    "        self.constraints = {}\n",
    "        self.cnt = 0\n",
    "\n",
    "    def STRING(self, arg):\n",
    "        # remove \" \"\n",
    "        return arg[1:-1] \n",
    "    \n",
    "    def BOOLEAN(self, arg):\n",
    "        return bool(arg)\n",
    "    \n",
    "    def INT(self, arg):\n",
    "        # can be negative\n",
    "        return int(arg)\n",
    "    \n",
    "    def FLOAT(self, arg):\n",
    "        return float(arg)\n",
    "    \n",
    "    def NATURAL_NUMBER(self, number): \n",
    "        # represents number of duplications\n",
    "        return int(number)\n",
    "    \n",
    "    def USER_VALUE(self, arg):\n",
    "        # get the variable name\n",
    "        variable = arg[2:-2]\n",
    "        # extract the actual value supplied by the user - can be of any type.\n",
    "        return self.render_funcs[variable](self.match) \n",
    "    \n",
    "    def value(self, args): \n",
    "        # one argument encased in a list\n",
    "        return args[0]\n",
    "    \n",
    "    def attribute(self, args): \n",
    "        # if an optional token was not parsed, None is placed in the parse tree.\n",
    "        # if type and value are not allowed, then None is entered manualy.\n",
    "        if self.component == \"P\": \n",
    "            attr_name = args[0]\n",
    "            type, value = None, None\n",
    "        else:\n",
    "            attr_name, type, value = args\n",
    "        # pass a tuple of attr_name, required type, required value.\n",
    "        return (attr_name, type, value)\n",
    "    \n",
    "    def attributes(self, attributes): # a list of triples \n",
    "        # return a packed list of the attribute names.\n",
    "        attr_names, constraints = {}, {}\n",
    "        for attribute in attributes:\n",
    "            # will be added to the graph itself\n",
    "            attr_name, type, value = attribute\n",
    "            if self.component == \"LHS\":\n",
    "                attr_names[str(attr_name)] = None \n",
    "                # will be added to the condition function\n",
    "                constraints[str(attr_name)] = (type, value) \n",
    "            else:\n",
    "                attr_names[str(attr_name)] = value\n",
    "\n",
    "        return (attr_names, constraints)\n",
    "\n",
    "    def multi_connection(self, args): # +\n",
    "        # return the list of attributes(strings), add a special attribute to denote number of duplications.\n",
    "        number, attributes = args\n",
    "        if attributes == None:\n",
    "            attributes = ({},{})\n",
    "        # add a special atrribute to handle duplications during construction\n",
    "        attributes[0][\"$dup\"] = number \n",
    "        return attributes\n",
    "\n",
    "    def connection(self, args): \n",
    "        # (tuple of dicts: attributes, constraints. attributes is of the form: attribute -> val)\n",
    "        attributes = args[0]\n",
    "        if attributes == None:\n",
    "            attributes = ({},{})\n",
    "        # add a special atrribute to handle duplications during construction\n",
    "        attributes[0][\"$dup\"] = 1\n",
    "        return (attributes, True)\n",
    "\n",
    "    def ANONYMUS(self, _): #\n",
    "        # return a dedicated name for anonymus (string), and an empty indices list.\n",
    "        x = \"_\" + str(self.cnt)\n",
    "        self.cnt += 1\n",
    "        return (x, [])\n",
    "\n",
    "    def index_vertex(self, args):\n",
    "        # return the main name of the vertex, and a list of the indices specified.\n",
    "        main_name_tup, *numbers = args #numbers is a list\n",
    "        return (main_name_tup[0], list(numbers))\n",
    "    \n",
    "    def NAMED_VERTEX(self, name):\n",
    "        # return the main name of the vertex, and an empty indices list.\n",
    "        return (str(name), [])\n",
    "\n",
    "    def vertex(self, args): # (vertex_tuple: tuple, attributes: list)\n",
    "        # attributes is a empty list/ a list containing a tuple: (names dict, constraints dict)\n",
    "        vertex_tuple, *attributes = args \n",
    "        name, indices_list = vertex_tuple\n",
    "\n",
    "        # create new name\n",
    "        indices = \",\".join([str(num) for num in indices_list])\n",
    "        if len(indices) == 0:\n",
    "            new_name = str(name)\n",
    "        else:\n",
    "            new_name =  name + \"<\" + indices + \">\" \n",
    "\n",
    "        # no attributes to handle\n",
    "        if attributes[0] == None:\n",
    "            return (new_name, {})\n",
    "        \n",
    "        # now that we have the vertex name we add the attribute constraints:\n",
    "        # vertices may appear multiple times in LHS thus we unite the constraints. We assume there cannot be contradicting constraints.\n",
    "        attribute_names, constraints = attributes[0] \n",
    "        # the second element of the tuple is the constraints dict: attr_name -> (value,type)\n",
    "        if self.component == \"LHS\":\n",
    "            if new_name not in self.constraints.keys():\n",
    "                self.constraints[new_name] = {}\n",
    "            self.constraints[new_name] = self.constraints[new_name] | constraints \n",
    "        return (new_name, attribute_names)\n",
    "\n",
    "    def pattern(self, args):\n",
    "        # 1) unpack lists of vertices and connections.\n",
    "        vertex, *rest = args\n",
    "        conn, vertices = list(rest)[::2], list(rest)[1::2]\n",
    "        vertices.insert(0,vertex)\n",
    "        # 2) create a networkX graph:\n",
    "            # Future feature: if there is a special attribute with TRUE (deterministic), dumplicate the connection $dup times.\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # simplified vertion - ignore duplications\n",
    "        G.add_nodes_from(vertices)\n",
    "        edge_list = []\n",
    "        for i,edge in enumerate(conn):\n",
    "            # for now the duplication feature is not included so we remove the $dup attribute\n",
    "            # we handeled None in the connection rule.\n",
    "            attribute_names, constraints = edge[0]\n",
    "            attribute_names.pop(\"$dup\", 0)\n",
    "            # ignore edge[1] - determinism flag. edge[0] is the tuple of dicts of attributes.\n",
    "            vertex_name_pos = 0 # each item in vertices is a tuple (vertex_name, attrs)\n",
    "            edge_list.append((vertices[i][vertex_name_pos], vertices[i+1][vertex_name_pos], attribute_names)) \n",
    "\n",
    "            # add constraints - we assume an edge only appears once in LHS\n",
    "            if self.component == \"LHS\":\n",
    "                filtered_cons = dict(filter(lambda tup: not tup[1] == (None, None), constraints.items()))\n",
    "                # check if filtered_cons is not empty - there are concrete constraints\n",
    "                if filtered_cons: \n",
    "                    self.constraints[str(vertices[i][vertex_name_pos]) + \"->\" + str(vertices[i+1][vertex_name_pos])] = filtered_cons\n",
    "\n",
    "        # more complex vertion - duplications\n",
    "        # create a recursive function that adds the vertices and edges, \n",
    "        # that calls itself by the number of duplications on each level.\n",
    "\n",
    "        G.add_edges_from(edge_list)\n",
    "        return G\n",
    "\n",
    "    def empty(self, _):\n",
    "        return nx.DiGraph()\n",
    "    \n",
    "    def patterns(self, args):\n",
    "        g, *graphs = args\n",
    "        graphs.insert(0,g)\n",
    "        # unite all the patterns into a single graph\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # dict of dicts (node_name -> attribute -> None/someValue)\n",
    "        combined_attributes = dict() \n",
    "        new_nodes = []\n",
    "        new_edges = []\n",
    "        for graph in graphs:\n",
    "            for node in graph.nodes:\n",
    "                if node not in combined_attributes.keys():\n",
    "                    combined_attributes[node] = {}\n",
    "                combined_attributes[node] = combined_attributes[node] | graph.nodes.data()[node]\n",
    "                #unite the dicts for each\n",
    "                new_nodes.append(node) \n",
    "            for edge in graph.edges:\n",
    "                # we assumed edges cannot appear more than once in LHS\n",
    "                combined_attributes[edge[0] + \"->\" + edge[1]] = graph.edges[edge[0],edge[1]]\n",
    "                new_edges.append(edge)\n",
    "        # filtered_attr = dict(filter(lambda _,value: not value == (None, None), combined_attributes.items()))\n",
    "        G.add_nodes_from([(node, combined_attributes[node]) for node in new_nodes])\n",
    "        G.add_edges_from([(node1, node2, combined_attributes[node1 + \"->\" + node2]) for (node1,node2) in new_edges])\n",
    "        \n",
    "        #sent as a module output and replaces condition.\n",
    "        return (G, copy.deepcopy(self.constraints)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Application\n",
    "The following function applies the transformer on an LHS-formatted string provided by the user, to extract the constraints and the resulting networkx greaph. Then it unites the constraints with the constraints given in the *condition* function supplied by the user, so that they will be inforced together later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#TODO: Once Dean approves the solution, remove the condition parameter from this func and from all caller functions\n",
    "# (it is only used for the POC, to avoid changing the entire code before even approving the code)\n",
    "\n",
    "def lhs_to_graph(lhs: str, condition=lambda x: True, debug=False):\n",
    "    \"\"\"\n",
    "    Converts a LHS string to a networkx graph and extracts constraints.\n",
    "\n",
    "    Args:\n",
    "    - lhs: str - a string representing the LHS of a rule.\n",
    "    - debug: bool - if True, returns the parse tree and the collections tree, instead of the graphs.\n",
    "\n",
    "    Returns:\n",
    "    - nx.DiGraph - the graph representing the single nodes pattern.\n",
    "    - nx.DiGraph - the graph representing the collections pattern.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lhs_strs = lhs.split(';')\n",
    "        assert len(lhs_strs) <= 2 and len(lhs_strs) >= 1 # at most 2 parts: single nodes and collections, at least 1 part (single nodes)\n",
    "\n",
    "        # Single nodes\n",
    "        single_nodes_lhs = lhs_strs[0]\n",
    "        single_nodes_tree = lhs_parser.parse(single_nodes_lhs)\n",
    "        if debug:\n",
    "            return single_nodes_tree, '', None # collections_tree == ''\n",
    "        single_nodes_graph, single_nodes_constraints = graphRewriteTransformer(component=\"LHS\").transform(single_nodes_tree)\n",
    "        _process_constraints(single_nodes_graph, single_nodes_constraints)\n",
    "\n",
    "        # Collections\n",
    "        if len(lhs_strs) == 2: # collections part exists\n",
    "            collections_lhs = lhs_strs[1]\n",
    "            collections_tree = lhs_parser.parse(collections_lhs)\n",
    "            if debug:\n",
    "                return single_nodes_tree, collections_tree, None\n",
    "            collections_graph, collections_constraints = graphRewriteTransformer(component=\"LHS\").transform(collections_tree)\n",
    "            _process_constraints(collections_graph, collections_constraints)\n",
    "        else: # collections is empty\n",
    "            collections_graph = nx.DiGraph()\n",
    "\n",
    "        return single_nodes_graph, collections_graph, condition # TODO: remove condition once Dean approves the solution - it is only used for working on the POC\n",
    "\n",
    "    except (BaseException, UnexpectedCharacters, UnexpectedToken) as e:\n",
    "        raise GraphRewriteException('Unable to convert LHS: {}'.format(e))\n",
    "\n",
    "\n",
    "def _process_constraints(graph: nx.DiGraph, constraints: dict):\n",
    "    \"\"\"\n",
    "    Processes the constraints for the given graph and inserts the attribute values into the nodes/edges.\n",
    "\n",
    "    Args:\n",
    "    - graph: nx.DiGraph - the graph whose constraints are to be processed.\n",
    "    - constraints: dict - a dictionary of constraints from the graphRewriteTransformer.\n",
    "\n",
    "    This method directly modifies the graph to include the attribute values in the nodes/edges, using the same format as\n",
    "    attributes in nx.DiGraphs objects (i.e. a dictionary of dictionaries, for example: {graph_obj: {attr_name: attr_value}}).\n",
    "    \"\"\"\n",
    "    for graph_obj, attr_constraints in constraints.items():\n",
    "        for attr_name, (attr_type_str, attr_value) in attr_constraints.items():\n",
    "            converted_value = _convert_value(attr_value, attr_type_str)\n",
    "\n",
    "            if graph_obj in graph.nodes:\n",
    "                graph.nodes[graph_obj][attr_name] = converted_value\n",
    "            else: # Edge\n",
    "                node1, node2 = graph_obj.split(\"->\")\n",
    "                graph.edges[node1, node2][attr_name] = converted_value\n",
    "\n",
    "\n",
    "def _convert_value(attr_value: str, attr_type_str: str):\n",
    "    \"\"\"\n",
    "    Converts the attribute value to the specified type. If the type is unrecognized,\n",
    "    it wraps the value in `TypedAttribute`. If the type is unmentioned (None), it \n",
    "    returns the value as is.\n",
    "\n",
    "    Args:\n",
    "    - attr_value: str - the attribute value as a string.\n",
    "    - attr_type_str: str - the string representing the expected type.\n",
    "\n",
    "    Returns:\n",
    "    - The converted value or an instance of the `TypedAttribute` class.\n",
    "    \"\"\"\n",
    "    if attr_value is None:\n",
    "        return None\n",
    "\n",
    "    # We need to take the type into account when examining the value\n",
    "    type_mapping = {\n",
    "        \"str\": str,\n",
    "        \"int\": int,\n",
    "        \"float\": float,\n",
    "        \"bool\": bool\n",
    "    }\n",
    "\n",
    "    if attr_type_str is None:\n",
    "        return attr_value\n",
    "    \n",
    "    # TODO: show Dean - do we want to keep ignoring unrecognized types, or should we raise an exception?\n",
    "    if attr_type_str not in type_mapping:\n",
    "        return TypedAttribute(attr_type_str, str(attr_value), False)\n",
    "\n",
    "    return TypedAttribute(attr_type_str, type_mapping[attr_type_str](attr_value))\n",
    "\n",
    "class TypedAttribute:\n",
    "    \"\"\"\n",
    "    Wrapper class for attributes with a specified type. \n",
    "    This class holds the attribute value and its type.\n",
    "    \"\"\"\n",
    "    def __init__(self, attr_type_str, attr_value, is_recognized=True):\n",
    "        self._attr_type_str = attr_type_str\n",
    "        self._attr_value = attr_value\n",
    "        self._is_recognized = is_recognized\n",
    "\n",
    "    # Transparent access to the value\n",
    "    def __getitem__(self):\n",
    "        return self._attr_value\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        # Compare both the value and the type to another value\n",
    "        if self._is_recognized:\n",
    "            return self._attr_value == other and self._attr_type_str == other.__class__.__name__\n",
    "        return self._attr_value == other\n",
    "    \n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self._attr_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "Note that throughout these tests, we use the naive condition which returns True for all matches. We chose to do that since this module is all about parsing, which is not affected by the condition.\n",
    "The condition will be checked appropriately in the module that actually uses it, the Matcher module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_condition = lambda x: True\n",
    "res, _, _ = lhs_to_graph(\"a\", naive_condition)\n",
    "expected = _create_graph(['a'], [])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _, _ = lhs_to_graph(\"a->b\", naive_condition)\n",
    "expected = _create_graph(['a','b'], [('a','b')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _, _ = lhs_to_graph(\"a -> b\", naive_condition)\n",
    "expected = _create_graph(['a','b'], [('a','b')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _, _ = lhs_to_graph(\"a->b -> c\", naive_condition)\n",
    "expected = _create_graph(['a','b','c'], [('a','b'),('b','c')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _, _ = lhs_to_graph(\"a->b -> a\", naive_condition)\n",
    "expected = _create_graph(['a','b'], [('a','b'),('b','a')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "# anonymus vertices\n",
    "res, _, _ = lhs_to_graph(\"a->_->b->_\", naive_condition)\n",
    "expected = _create_graph(['a','b','_0','_1'], [('a','_0'),('_0','b'),('b','_1')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, _, _ = lhs_to_graph(\"a[x=5]\", naive_condition)\n",
    "expected = _create_graph([('a', {'x': 5})], [])\n",
    "assert(_graphs_equal(expected, res))\n",
    "\n",
    "res, _, _ = lhs_to_graph(\"a-[x=5]->b\", naive_condition)\n",
    "expected = _create_graph(['a', 'b'], [('a','b',{'x': 5})])\n",
    "assert(_graphs_equal(expected, res))\n",
    "\n",
    "res, _, _ = lhs_to_graph(\"a<1,2>[x=5, y: int = 6]\", naive_condition)\n",
    "expected = _create_graph([('a<1,2>',{'x':5, 'y':6})],[])\n",
    "assert(_graphs_equal(res,expected))\n",
    "\n",
    "res, _, _ = lhs_to_graph(\"a[a]-[x]->b[ b ] -> c[ c ]\", naive_condition)\n",
    "expected = _create_graph([('a',{'a':None}), ('b',{'b':None}), ('c',{'c':None})],[('a','b',{'x':None}),('b','c')])\n",
    "assert(_graphs_equal(res,expected))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmZsb3djaGFydCBUQgpyZWxbInJlbAp2YWw9cmVsYXRpb24iXQp6WyJ6CnZhbD1yZWxhdGlvbl9uYW1lIl0KcmVsIC0tPiB6Cg==\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patterns\n",
      "  pattern\n",
      "    vertex\n",
      "      rel\n",
      "      attributes\n",
      "        attribute\n",
      "          val\n",
      "          str\n",
      "          value\t\"relation\"\n",
      "    connection\tNone\n",
      "    vertex\n",
      "      z\n",
      "      attributes\n",
      "        attribute\n",
      "          val\n",
      "          str\n",
      "          value\t\"relation_name\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t2, _, _ = lhs_to_graph('''rel[val:str=\"relation\"]->z[val:str=\"relation_name\"]''',naive_condition,debug=True)\n",
    "g2, _, c = lhs_to_graph('''rel[val:str=\"relation\"]->z[val:str=\"relation_name\"]''' ,naive_condition)\n",
    "draw(g2)\n",
    "print(t2.pretty())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiple patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, _, _ = lhs_to_graph(\"a->b -> c, c-> d\", naive_condition) \n",
    "expected = _create_graph(['a','b','c','d'], [('a','b'),('b','c'),('c','d')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _, _ = lhs_to_graph(\"a->b -> c, d\", naive_condition) \n",
    "expected = _create_graph(['a','b','c', 'd'], [('a','b'),('b','c')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _, _ = lhs_to_graph(\"a->b[z] -> c[y], c[x=5]->b[r]\", naive_condition) \n",
    "expected = _create_graph(['a',('b',{\"z\":None, \"r\":None}),('c',{'x':5,'y':None})], [('a','b'),('b','c'),('c','b')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('c', {'type': 'course'})]\n",
      "[('s', {'type': 'student'}), ('c', {})]\n"
     ]
    }
   ],
   "source": [
    "res_single, res_collection, _ = lhs_to_graph('c[type=\"course\"];s[type=\"student\"]->c', naive_condition) \n",
    "print(res_single.nodes.data())\n",
    "print(res_collection.nodes.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
