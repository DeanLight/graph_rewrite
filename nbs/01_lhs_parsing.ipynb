{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LHS Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This module defines the grammar of the LHS that is given by the user to the *rewrite* function of the library.\n",
    "The module is also responsible for parsing of the pattern sent as LHS, into a networkX graph representing the template to search.\n",
    "\n",
    "The module converts the declerative constraints regarding the properties of the nodes and edges in the LHS, to imperative functions that are checked together with the 'condition' parameter of *rewrite*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lark.lark.Lark"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import copy\n",
    "from collections.abc import Callable\n",
    "import networkx as nx\n",
    "from lark import Transformer, Lark\n",
    "from lark import UnexpectedCharacters, UnexpectedToken\n",
    "from graph_rewrite.match_class import Match\n",
    "from graph_rewrite.core import GraphRewriteException\n",
    "from graph_rewrite.core import _create_graph,  _graphs_equal, draw"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar\n",
    "The grammar induces the allowed syntax of a legal LHS string that can be provided by the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "lhs_parser = Lark(r\"\"\"\n",
    "    %import common.INT -> INT \n",
    "    %import common.FLOAT -> FLOAT\n",
    "    %import common.ESCAPED_STRING -> STRING\n",
    "    %import common.WS -> WS\n",
    "    %ignore WS\n",
    "\n",
    "    NAMED_VERTEX: /[_a-zA-Z0-9]+/\n",
    "    ANONYMUS: \"_\"\n",
    "    ATTR_NAME: /[_a-zA-Z0-9]+/\n",
    "    TYPE:  \"int\" | \"str\" | \"bool\" | \"float\"\n",
    "    BOOLEAN: \"True\" | \"False\"\n",
    "    NATURAL_NUMBER: /[1-9][0-9]*/\n",
    "    INDEX: /[0-9]+/\n",
    "\n",
    "    value: FLOAT | INT | BOOLEAN | STRING\n",
    "\n",
    "    attribute: ATTR_NAME [\":\" TYPE] [\"=\" value]\n",
    "    attributes: \"[\" attribute (\",\" attribute)* \"]\"\n",
    "\n",
    "    multi_connection: \"-\" NATURAL_NUMBER [attributes] \"->\" \n",
    "    connection: [\"-\" attributes]\"->\"\n",
    "              | multi_connection\n",
    "    \n",
    "    index_vertex: NAMED_VERTEX \"<\" INDEX (\",\" INDEX)* \">\"\n",
    "\n",
    "    vertex: NAMED_VERTEX [attributes]\n",
    "    | index_vertex [attributes]\n",
    "    | ANONYMUS [attributes]\n",
    "\n",
    "    pattern: vertex (connection vertex)*\n",
    "    patterns: pattern (\";\" pattern)* # TODO: Replace \";\" with \",\" # Collections Feature\n",
    "\n",
    "    \"\"\", parser=\"lalr\", start='patterns' , debug=True)\n",
    "\n",
    "# multi_connection: \"-\" NATURAL_NUMBER \"+\" [attributes] \"->\"  - setting for the \"-num+->\" feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "The transformer is designed to return the networkX graph representing the patterns received by the user.\n",
    "\n",
    "For each branch, the appropriate method will be called with the children of the branch as its argument, and its return value will replace the matching node in the tree.\n",
    "\n",
    "The secondary task of the transformer is to collect the node/edge type and constant node/edge value constraints, such that they are added to the 'condition' parameter to be checked later. Thus, the lhsTransformer contains a python dictionary *constraints* which accumulates the constraints from all components of the parsed graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "RenderFunc = Callable[[Match], any] # type of a function to render a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "cnt:int = 0 # unique id for anonymous vertices\n",
    "class graphRewriteTransformer(Transformer):\n",
    "    def __init__(self, visit_tokens: bool = True, component: str = \"LHS\", match: Match = None, render_funcs: dict[str, RenderFunc] = {}) -> None:\n",
    "        super().__init__(visit_tokens)\n",
    "        # general\n",
    "        self.component = component\n",
    "        # RHS parameters\n",
    "        self.match = match\n",
    "        self.render_funcs = render_funcs\n",
    "        # LHS parameters\n",
    "        self.constraints = {}\n",
    "        self.cnt = 0\n",
    "\n",
    "    def STRING(self, arg):\n",
    "        # remove \" \"\n",
    "        return arg[1:-1] \n",
    "    \n",
    "    def BOOLEAN(self, arg):\n",
    "        return bool(arg)\n",
    "    \n",
    "    def INT(self, arg):\n",
    "        # can be negative\n",
    "        return int(arg)\n",
    "    \n",
    "    def FLOAT(self, arg):\n",
    "        return float(arg)\n",
    "    \n",
    "    def NATURAL_NUMBER(self, number): \n",
    "        # represents number of duplications\n",
    "        return int(number)\n",
    "    \n",
    "    def USER_VALUE(self, arg):\n",
    "        # get the variable name\n",
    "        variable = arg[2:-2]\n",
    "        # extract the actual value supplied by the user - can be of any type.\n",
    "        return self.render_funcs[variable](self.match) \n",
    "    \n",
    "    def value(self, args): \n",
    "        # one argument encased in a list\n",
    "        return args[0]\n",
    "    \n",
    "    def attribute(self, args): \n",
    "        # if an optional token was not parsed, None is placed in the parse tree.\n",
    "        # if type and value are not allowed, then None is entered manualy.\n",
    "        if self.component == \"P\": \n",
    "            attr_name = args[0]\n",
    "            type, value = None, None\n",
    "        else:\n",
    "            attr_name, type, value = args\n",
    "        # pass a tuple of attr_name, required type, required value.\n",
    "        return (attr_name, type, value)\n",
    "    \n",
    "    def attributes(self, attributes): # a list of triples \n",
    "        # return a packed list of the attribute names.\n",
    "        attr_names, constraints = {}, {}\n",
    "        for attribute in attributes:\n",
    "            # will be added to the graph itself\n",
    "            attr_name, type, value = attribute\n",
    "            if self.component == \"LHS\":\n",
    "                attr_names[str(attr_name)] = None \n",
    "                # will be added to the condition function\n",
    "                constraints[str(attr_name)] = (type, value) \n",
    "            else:\n",
    "                attr_names[str(attr_name)] = value\n",
    "\n",
    "        return (attr_names, constraints)\n",
    "\n",
    "    def multi_connection(self, args): # +\n",
    "        # return the list of attributes(strings), add a special attribute to denote number of duplications.\n",
    "        number, attributes = args\n",
    "        if attributes == None:\n",
    "            attributes = ({},{})\n",
    "        # add a special atrribute to handle duplications during construction\n",
    "        attributes[0][\"$dup\"] = number \n",
    "        return attributes\n",
    "\n",
    "    def connection(self, args): \n",
    "        # (tuple of dicts: attributes, constraints. attributes is of the form: attribute -> val)\n",
    "        attributes = args[0]\n",
    "        if attributes == None:\n",
    "            attributes = ({},{})\n",
    "        # add a special atrribute to handle duplications during construction\n",
    "        attributes[0][\"$dup\"] = 1\n",
    "        return (attributes, True)\n",
    "\n",
    "    def ANONYMUS(self, _): #\n",
    "        # return a dedicated name for anonymus (string), and an empty indices list.\n",
    "        x = \"_\" + str(self.cnt)\n",
    "        self.cnt += 1\n",
    "        return (x, [])\n",
    "\n",
    "    def index_vertex(self, args):\n",
    "        # return the main name of the vertex, and a list of the indices specified.\n",
    "        main_name_tup, *numbers = args #numbers is a list\n",
    "        return (main_name_tup[0], list(numbers))\n",
    "    \n",
    "    def NAMED_VERTEX(self, name):\n",
    "        # return the main name of the vertex, and an empty indices list.\n",
    "        return (str(name), [])\n",
    "\n",
    "    def vertex(self, args): # (vertex_tuple: tuple, attributes: list)\n",
    "        # attributes is a empty list/ a list containing a tuple: (names dict, constraints dict)\n",
    "        vertex_tuple, *attributes = args \n",
    "        name, indices_list = vertex_tuple\n",
    "\n",
    "        # create new name\n",
    "        indices = \",\".join([str(num) for num in indices_list])\n",
    "        if len(indices) == 0:\n",
    "            new_name = str(name)\n",
    "        else:\n",
    "            new_name =  name + \"<\" + indices + \">\" \n",
    "\n",
    "        # no attributes to handle\n",
    "        if attributes[0] == None:\n",
    "            return (new_name, {})\n",
    "        \n",
    "        # now that we have the vertex name we add the attribute constraints:\n",
    "        # vertices may appear multiple times in LHS thus we unite the constraints. We assume there cannot be contradicting constraints.\n",
    "        attribute_names, constraints = attributes[0] \n",
    "        # the second element of the tuple is the constraints dict: attr_name -> (value,type)\n",
    "        if self.component == \"LHS\":\n",
    "            if new_name not in self.constraints.keys():\n",
    "                self.constraints[new_name] = {}\n",
    "            self.constraints[new_name] = self.constraints[new_name] | constraints \n",
    "        return (new_name, attribute_names)\n",
    "\n",
    "    def pattern(self, args):\n",
    "        # 1) unpack lists of vertices and connections.\n",
    "        vertex, *rest = args\n",
    "        conn, vertices = list(rest)[::2], list(rest)[1::2]\n",
    "        vertices.insert(0,vertex)\n",
    "        # 2) create a networkX graph:\n",
    "            # Future feature: if there is a special attribute with TRUE (deterministic), dumplicate the connection $dup times.\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # simplified vertion - ignore duplications\n",
    "        G.add_nodes_from(vertices)\n",
    "        edge_list = []\n",
    "        for i,edge in enumerate(conn):\n",
    "            # for now the duplication feature is not included so we remove the $dup attribute\n",
    "            # we handeled None in the connection rule.\n",
    "            attribute_names, constraints = edge[0]\n",
    "            attribute_names.pop(\"$dup\", 0)\n",
    "            # ignore edge[1] - determinism flag. edge[0] is the tuple of dicts of attributes.\n",
    "            vertex_name_pos = 0 # each item in vertices is a tuple (vertex_name, attrs)\n",
    "            edge_list.append((vertices[i][vertex_name_pos], vertices[i+1][vertex_name_pos], attribute_names)) \n",
    "\n",
    "            # add constraints - we assume an edge only appears once in LHS\n",
    "            if self.component == \"LHS\":\n",
    "                filtered_cons = dict(filter(lambda tup: not tup[1] == (None, None), constraints.items()))\n",
    "                # check if filtered_cons is not empty - there are concrete constraints\n",
    "                if filtered_cons: \n",
    "                    self.constraints[str(vertices[i][vertex_name_pos]) + \"->\" + str(vertices[i+1][vertex_name_pos])] = filtered_cons\n",
    "\n",
    "        # more complex vertion - duplications\n",
    "        # create a recursive function that adds the vertices and edges, \n",
    "        # that calls itself by the number of duplications on each level.\n",
    "\n",
    "        G.add_edges_from(edge_list)\n",
    "        return G\n",
    "\n",
    "    def empty(self, _):\n",
    "        return nx.DiGraph()\n",
    "    \n",
    "    def patterns(self, args):\n",
    "        g, *graphs = args\n",
    "        graphs.insert(0,g)\n",
    "        # unite all the patterns into a single graph\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # dict of dicts (node_name -> attribute -> None/someValue)\n",
    "        combined_attributes = dict() \n",
    "        new_nodes = []\n",
    "        new_edges = []\n",
    "        for graph in graphs:\n",
    "            for node in graph.nodes:\n",
    "                if node not in combined_attributes.keys():\n",
    "                    combined_attributes[node] = {}\n",
    "                combined_attributes[node] = combined_attributes[node] | graph.nodes.data()[node]\n",
    "                #unite the dicts for each\n",
    "                new_nodes.append(node) \n",
    "            for edge in graph.edges:\n",
    "                # we assumed edges cannot appear more than once in LHS\n",
    "                combined_attributes[edge[0] + \"->\" + edge[1]] = graph.edges[edge[0],edge[1]]\n",
    "                new_edges.append(edge)\n",
    "        # filtered_attr = dict(filter(lambda _,value: not value == (None, None), combined_attributes.items()))\n",
    "        G.add_nodes_from([(node, combined_attributes[node]) for node in new_nodes])\n",
    "        G.add_edges_from([(node1, node2, combined_attributes[node1 + \"->\" + node2]) for (node1,node2) in new_edges])\n",
    "        \n",
    "        #sent as a module output and replaces condition.\n",
    "        return (G, copy.deepcopy(self.constraints)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Application\n",
    "The following function applies the transformer on an LHS-formatted string provided by the user, to extract the constraints and the resulting networkx greaph. Then it unites the constraints with the constraints given in the *condition* function supplied by the user, so that they will be inforced together later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lhs_to_graph(lhs: str, condition = None,debug=False):\n",
    "    \"\"\"Given an LHS pattern and a condition function, return the directed graph represented by the pattern, \n",
    "    along with an updated condition function that combines the original constraints and the new value and type constraints\n",
    "    deriving from the pattern.\n",
    "\n",
    "    Args:\n",
    "        lhs (string): A string in lhs format \n",
    "        condition (lambda: Match -> bool): A function supplied by the user specifying additional \n",
    "                                           constraints on the graph components.\n",
    "\n",
    "    Returns:\n",
    "        DiGraph, lambda: Match->bool: a networkx graph that is the graph represented by the pattern, \n",
    "                                      and an extended condition function as mentioned above.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        #TODO: add split for collections and also return lhs_collections_graph # Collections Feature\n",
    "        '''\n",
    "        lhs_strs = lhs.split(';')\n",
    "        assert len(lhs_strs) <= 2 and len(lhs_strs) >= 1\n",
    "        lhs = lhs_split[0]\n",
    "        if (len(str_split) == 2):\n",
    "            lhs_collections = lhs_split[1]\n",
    "        else:\n",
    "            lhs_collections = ''\n",
    "        if lhs_collections != '':\n",
    "            collections_tree = lhs_parser.parse(lhs)\n",
    "        '''\n",
    "        tree = lhs_parser.parse(lhs)\n",
    "        if debug:\n",
    "            return tree, None # , collections_tree # Collections Feature\n",
    "        final_graph, constraints = graphRewriteTransformer(component=\"LHS\").transform(tree)\n",
    "        # constraints is a dictionary: vertex/edge -> {attr_name: (value, type), ...}\n",
    "        # collections_graph, collections_constraints = graphRewriteTransformer(component=\"LHS\").transform(collections_tree) # Collections Feature\n",
    "        # TODO: combine constraints and collection_constraints? # Collections Feature\n",
    "\n",
    "        # add the final constraints to the \"condition\" function\n",
    "        def type_condition(match: Match):\n",
    "            flag = True\n",
    "            for graph_obj in constraints.keys():\n",
    "                obj_constraints = constraints[graph_obj]\n",
    "                for attr_name in obj_constraints.keys():\n",
    "                    required_type_str, required_value = obj_constraints[attr_name]\n",
    "\n",
    "                    # check value constraint\n",
    "                    if required_value != None:\n",
    "                        if not hasattr(required_value, '__eq__') or (not required_value == match[graph_obj][attr_name]):\n",
    "                            flag = False\n",
    "                    \n",
    "                    # check type constraint only of value was not checked\n",
    "                    str_to_type = {\"str\":str, \"float\":float, \"int\":int, \"bool\":bool}\n",
    "                    if not required_type_str == None:\n",
    "                        required_type = str_to_type[required_type_str]\n",
    "                    else:\n",
    "                        required_type = None\n",
    "                    \n",
    "                    if required_type_str != None and not isinstance(match[graph_obj][attr_name], required_type):\n",
    "                        flag = False\n",
    "    \n",
    "            # True <=> the match satisfies all the constraints.\n",
    "            if condition == None:\n",
    "                return flag\n",
    "            else:\n",
    "                return flag and condition(match) \n",
    "                \n",
    "        return final_graph, type_condition # , collections_graph # Collections Feature\n",
    "    except (BaseException, UnexpectedCharacters, UnexpectedToken) as e:\n",
    "        raise GraphRewriteException('Unable to convert LHS: {}'.format(e))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "Note that throughout these tests, we use the naive condition which returns True for all matches. We chose to do that since this module is all about parsing, which is not affected by the condition.\n",
    "The condition will be checked appropriately in the module that actually uses it, the Matcher module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_condition = lambda x: True\n",
    "res, _ = lhs_to_graph(\"a\", naive_condition)\n",
    "expected = _create_graph(['a'], [])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _ = lhs_to_graph(\"a->b\", naive_condition)\n",
    "expected = _create_graph(['a','b'], [('a','b')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _ = lhs_to_graph(\"a -> b\", naive_condition)\n",
    "expected = _create_graph(['a','b'], [('a','b')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _ = lhs_to_graph(\"a->b -> c\", naive_condition)\n",
    "expected = _create_graph(['a','b','c'], [('a','b'),('b','c')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _ = lhs_to_graph(\"a->b -> a\", naive_condition)\n",
    "expected = _create_graph(['a','b'], [('a','b'),('b','a')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "# anonymus vertices\n",
    "res, _ = lhs_to_graph(\"a->_->b->_\", naive_condition)\n",
    "expected = _create_graph(['a','b','_0','_1'], [('a','_0'),('_0','b'),('b','_1')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, _ = lhs_to_graph(\"a[x=5]\", naive_condition)\n",
    "expected = _create_graph([('a', {'x':None})], [])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _ = lhs_to_graph(\"a-[x=5]->b\", naive_condition)\n",
    "expected = _create_graph(['a', 'b'], [('a','b',{'x':None})])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _ = lhs_to_graph(\"a<1,2>[x=5, y: int = 6]\", naive_condition)\n",
    "expected = _create_graph([('a<1,2>',{'x':None, 'y':None})],[])\n",
    "assert(_graphs_equal(res,expected))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _ = lhs_to_graph(\"a[a]-[x]->b[ b ] -> c[ c ]\", naive_condition)\n",
    "expected = _create_graph([('a',{'a':None}), ('b',{'b':None}), ('c',{'c':None})],[('a','b',{'x':None}),('b','c')])\n",
    "assert(_graphs_equal(res,expected))\n",
    "#_plot_graph(res)\n",
    "\n",
    "# res, _ = lhs_to_graph(\"a<1>[x=5, y: int = 6]\", condition)\n",
    "# expected = _create_graph([('a<1>',{'x':None, 'y':None})],[])\n",
    "# assert(_graphs_equal(res,expected))\n",
    "#_plot_graph(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmZsb3djaGFydCBUQgpyZWxbInJlbAp2YWw9Tm9uZSJdCnpbInoKdmFsPU5vbmUiXQpyZWwgLS0+IHoK\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patterns\n",
      "  pattern\n",
      "    vertex\n",
      "      rel\n",
      "      attributes\n",
      "        attribute\n",
      "          val\n",
      "          str\n",
      "          value\t\"relation\"\n",
      "    connection\tNone\n",
      "    vertex\n",
      "      z\n",
      "      attributes\n",
      "        attribute\n",
      "          val\n",
      "          str\n",
      "          value\t\"relation_name\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t2,_ = lhs_to_graph('''rel[val:str=\"relation\"]->z[val:str=\"relation_name\"]''',debug=True)\n",
    "g2,c = lhs_to_graph('''rel[val:str=\"relation\"]->z[val:str=\"relation_name\"]''')\n",
    "draw(g2)\n",
    "print(t2.pretty())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiple patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, _ = lhs_to_graph(\"a->b -> c; c-> d\", naive_condition) # TODO: Replace \";\" with \",\" # Collections Feature\n",
    "expected = _create_graph(['a','b','c','d'], [('a','b'),('b','c'),('c','d')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _ = lhs_to_graph(\"a->b -> c; d\", naive_condition) # TODO: Replace \";\" with \",\" # Collections Feature\n",
    "expected = _create_graph(['a','b','c', 'd'], [('a','b'),('b','c')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _ = lhs_to_graph(\"a->b[z] -> c[y]; c[x=5]->b[r]\", naive_condition) # TODO: Replace \";\" with \",\" # Collections Feature\n",
    "expected = _create_graph(['a',('b',{\"z\":None, \"r\":None}),('c',{'x':None,'y':None})], [('a','b'),('b','c'),('c','b')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
