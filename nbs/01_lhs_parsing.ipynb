{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp lhs\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lark\n",
    "# %pip install networkx\n",
    "from lark import Lark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LHS parser ##\n",
    "\n",
    "parsing of the pattern sent as lhs, into a networkX graph representing the template to search.\n",
    "\n",
    "The module converts the declerative constraints regarding the properties of the nodes and edges in the LHS, to imperative functions that are checked together with the 'condition' parameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs_parser = Lark(r\"\"\"\n",
    "    %import common.INT -> INT \n",
    "    %import common.FLOAT -> FLOAT\n",
    "    %import common.ESCAPED_STRING -> STRING\n",
    "    %import common.WS -> WS\n",
    "    %ignore WS\n",
    "\n",
    "    NAMED_VERTEX: /[a-zA-Z0-9]+/\n",
    "    ANONYMUS: \"_\"\n",
    "    ATTR_NAME: /[a-zA-Z0-9]+/\n",
    "    TYPE:  \"int\" | \"string\"\n",
    "    BOOLEAN: \"True\" | \"False\"\n",
    "    NATURAL_NUMBER: /[1-9][0-9]?/\n",
    "    INDEX: /[0-9]+/\n",
    "\n",
    "    value: FLOAT | STRING | INT | BOOLEAN\n",
    "\n",
    "    attribute: ATTR_NAME [\":\" TYPE] [\"=\" value]\n",
    "    attributes: \"[\" attribute (\",\" attribute)* \"]\"\n",
    "\n",
    "    multi_connection: \"-\" NATURAL_NUMBER [attributes] \"->\" \n",
    "    connection: \"-\" [attributes \"-\"] \">\"\n",
    "              | multi_connection\n",
    "    \n",
    "    index_vertex: NAMED_VERTEX \"<\" INDEX (\",\" INDEX)* \">\"\n",
    "\n",
    "    vertex: NAMED_VERTEX [attributes]\n",
    "    | index_vertex [attributes]\n",
    "    | ANONYMUS [attributes]\n",
    "\n",
    "    pattern: vertex (connection vertex)*\n",
    "    patterns: pattern (\";\" pattern)*\n",
    "\n",
    "    \"\"\", parser=\"lalr\", start='patterns' , debug=True)\n",
    "\n",
    "# multi_connection: \"-\" NATURAL_NUMBER \"+\" [attributes] \"->\"  - setting for the \"-num+->\" feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "The transformer is designed to return the networkX graph representing the patterns.\n",
    "\n",
    "For each branch, the appropriate method will be called with the children of the branch as its argument, and its return value will replace the branch in the tree.\n",
    "\n",
    "The secondary task of the transformer is to collect the node/edge type and constant node/edge value constraints, such that they are added to the 'condition' parameter to be checked later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import itertools\n",
    "import copy\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "cnt:int = 0 # unique id for anonymous vertices\n",
    "from lark import Tree, Transformer\n",
    "class lhsTransformer(Transformer):\n",
    "    def __init__(self, visit_tokens: bool = True) -> None:\n",
    "        super().__init__(visit_tokens)\n",
    "        self.constraints = {}\n",
    "        self.cnt = 0\n",
    "\n",
    "    def STRING(self, arg):\n",
    "        return arg[1:-1] # remove \" \"\n",
    "    \n",
    "    def BOOLEAN(self, arg):\n",
    "        return bool(arg)\n",
    "    \n",
    "    def INT(self, arg): # can be negative\n",
    "        return int(arg)\n",
    "    \n",
    "    def FLOAT(self, arg):\n",
    "        return float(arg)\n",
    "    \n",
    "    def NATURAL_NUMBER(self, number): # for duplications\n",
    "        return int(number)\n",
    "    \n",
    "    def value(self,arg):\n",
    "        return arg\n",
    "    \n",
    "    def attribute(self, args): #(attr_name, *rest):\n",
    "        attr_name = args[0]\n",
    "        if len(args) == 3:\n",
    "            type, value = args[1:]\n",
    "        elif len(args) == 2:\n",
    "            # CHANGE!\n",
    "            print(\"parsed: \" + args[1])\n",
    "            value = None\n",
    "            type = None\n",
    "        # pass a tuple of attr_name, required type, required value.\n",
    "        return (attr_name, type, value) # constraints are handled in other transformer.\n",
    "    \n",
    "    def attributes(self, attributes): # a list of triples \n",
    "        # return a packed list of the attribute names.\n",
    "        attr_names, constraints = {}, {}\n",
    "        for attribute in attributes:\n",
    "            # print(attribute)\n",
    "            attr_names[str(attribute[0])] = None # will be added to the graph itself\n",
    "            constraints[attribute[0]] = (attribute[1], attribute[2]) # will be added to the condition function\n",
    "        return (attr_names, constraints)\n",
    "\n",
    "    def multi_connection(self, args): # +\n",
    "        # return the list of attributes(strings), add a special attribute to denote number of duplications.\n",
    "        #   for \"-+->\" implementation also return FALSE if \"+\" is parsed (indicating that the connection is not deterministic)\n",
    "        number, attributes = args\n",
    "        attributes[0][\"$dup\"] = number # removed in graph construction\n",
    "        return attributes\n",
    "\n",
    "    def connection(self, attributes): # (dict of attributes, constraints: attribute -> (val,type))\n",
    "        # return the packed list of attributes received, num_duplications = 1, is_deterministic = True\n",
    "        attributes[0][\"$dup\"] = 1\n",
    "        return (attributes, True)\n",
    "\n",
    "    def ANONYMUS(self): #\n",
    "        # return a dedicated name for anonymus (string), and an empty list.\n",
    "        x = (\"$\" + str(cnt), [])\n",
    "        self.cnt += 1\n",
    "        return x\n",
    "\n",
    "    def index_vertex(self, args):\n",
    "        # return the main name of the vertex, and a list of the indices specified.\n",
    "        main_name_tup, *numbers = args #numbers is a list\n",
    "        # print(main_name_tup)\n",
    "        # print(numbers)\n",
    "        return (main_name_tup[0], list(numbers))\n",
    "    \n",
    "    def NAMED_VERTEX(self, name):\n",
    "        # return the main name of the vertex, and an empty list.\n",
    "        return (name, [])\n",
    "\n",
    "    def vertex(self, args): # (vertex_tuple: tuple, attributes: dict = {})\n",
    "        # return arguments\n",
    "        vertex_tuple, *attributes = args # attributes is a empty list/ a list containing a tuple: (names dict, constraints dict)\n",
    "        name, indices_list = vertex_tuple \n",
    "        if indices_list == None:\n",
    "            indices_list = []\n",
    "        indices = \",\".join([str(num) for num in indices_list])\n",
    "        new_name = name + \"<\"\n",
    "        new_name = new_name + indices + \">\" # numbers are strings, no convertion needed.\n",
    "        if len(attributes) == 0:\n",
    "            return (new_name, {})\n",
    "        # now that we have the vertex name we add the attribute constraints:\n",
    "        # vertices may appear multiple times in LHS thus we unite the constraints. We assume there cannot be contradicting constraints.\n",
    "        self.constraints[new_name] = self.constraints[new_name] | attributes[0][1] # the second element of the tuple is the constraints dict: attr_name -> (value,type)\n",
    "        return (new_name, attributes[0][0]) #(string, dict)\n",
    "\n",
    "    def pattern(self, args):\n",
    "        # 1) unpack lists of vertices and connections.\n",
    "        vertex, *rest = args\n",
    "        conn, vertices = list(rest)[::2], list(rest)[1::2]\n",
    "        vertices.insert(0,vertex)\n",
    "        # print(vertices)\n",
    "        # print(conn)\n",
    "        # 2) create a networkX graph:\n",
    "            # Future feature: if there is a special attribute with TRUE (deterministic), dumplicate the connection $dup times.\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # simplified vertion - ignore duplications\n",
    "        G.add_nodes_from(vertices)\n",
    "        edge_list = []\n",
    "        for i,edge in enumerate(conn):\n",
    "            # for now the duplication feature is not included so we remove the $dup attribute\n",
    "            attribute_names, constraints = edge[0]\n",
    "            attribute_names.pop(\"$dup\", 0)\n",
    "            edge_list.append((vertices[i], vertices[i+1], attribute_names)) # ignore edge[1] - determinism flag. edge[0] is the tuple of dicts of attributes.\n",
    "            # add constraints - we assume an edge only appears once in LHS\n",
    "            self.constraints[str(vertices[i]) + \"->\" + str(vertices[i+1])] = constraints\n",
    "\n",
    "        # more complex vertion - duplications\n",
    "        # create a recursive function that adds the vertices and edges, \n",
    "        # that calls itself by the number of duplications on each level.\n",
    "        G.add_edges_from(edge_list)\n",
    "        return G\n",
    "\n",
    "    def patterns(self, g, *graphs):\n",
    "        patterns = list(graphs)\n",
    "        patterns.insert(0,g)\n",
    "        # unite all the patterns into a single graph\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        combined_attributes = dict() # dict of dicts (node_name -> attribute -> value)\n",
    "        new_nodes = []\n",
    "        new_edges = []\n",
    "        for graph in patterns:\n",
    "            for node in graph.nodes:\n",
    "                combined_attributes[node] = combined_attributes[node] | graph.nodes.data()[node]\n",
    "                new_nodes.append(node) #unite the dicts for each\n",
    "            for edge in graph.edges:\n",
    "                # we assumed edges cannot appear more than once in LHS\n",
    "                combined_attributes[edge[0] + \"->\" + edge[1]] = graph.edges[edge[0],edge[1]]\n",
    "                new_edges.append(edge)\n",
    "\n",
    "        G.add_nodes_from([(node, combined_attributes[node]) for node in new_nodes])\n",
    "        G.add_edges_from([(node1, node2, combined_attributes[node1 + \"->\" + node2]) for (node1,node2) in new_edges])\n",
    "\n",
    "        return (G, copy.deepcopy(self.constraints)) #sent as a module output and replaces condition."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from graph_rewrite.match_class import Match\n",
    "def lhs_to_graph(lhs: str, condition):\n",
    "    \"\"\"Given an LHS pattern and a condition function, return the directed graph represented by the pattern, \n",
    "    along with an updated condition function that combines the original constraints and the new value and type constraints\n",
    "    deriving from the pattern.\n",
    "\n",
    "    Args:\n",
    "        lhs (string): A string in lhs format \n",
    "        condition (lambda: Match -> bool): A function supplied by the user specifying additional \n",
    "                                           constraints on the graph components.\n",
    "\n",
    "    Returns:\n",
    "        DiGraph, lambda: Match->bool: a networkx graph that is the graph represented by the pattern, \n",
    "                                      and an extended condition function as mentioned above.\n",
    "    \"\"\"\n",
    "    tree = lhs_parser.parse(lhs)\n",
    "    final_graph, constraints = lhsTransformer().transform(tree)\n",
    "    # constraints is a dictionary: vertex/edge -> {attr_name: (value, type), ...}\n",
    "\n",
    "    # add the final constraints to the \"condition\" function\n",
    "    def type_condition(match: Match):\n",
    "        flag = True\n",
    "        for graph_obj in constraints.keys():\n",
    "            obj_constraints = constraints[graph_obj]\n",
    "            for attr_name in obj_constraints.keys():\n",
    "                required_type, required_value = obj_constraints[attr_name]\n",
    "\n",
    "                # check value constraint\n",
    "                if required_value != None:\n",
    "                    if not hasattr(required_value, '__eq__') or (not required_value == match[v][attr_name]):\n",
    "                        flag = False\n",
    "                \n",
    "                # check type constraint only of value was not checked\n",
    "                elif required_type != None and not isinstance(match[v][attr_name], required_type):\n",
    "                    flag = False\n",
    "\n",
    "        return flag and condition(match) # True <=> the match satisfies all the constraints.\n",
    "            \n",
    "    return final_graph, type_condition"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grammar Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree = lhs_parser.parse(\"aaaaa->b\") #.pretty(indent_str = \" $ \")\n",
    "# assert(tree != None)\n",
    "# assert(tree)\n",
    "# assert(tree == \n",
    "#     Tree(Token('RULE', 'patterns'),[\n",
    "#       Tree(Token('RULE', 'pattern'),[\n",
    "#         Tree(Token('RULE', 'vertex'),[\n",
    "#           Tree(Token('RULE', 'vertex'),[]),\n",
    "#           None\n",
    "#         ])\n",
    "#       ])\n",
    "#     ]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_rewrite.core import _create_graph\n",
    "\n",
    "def _graphs_equal(graph1, graph2):  \n",
    "    return (\n",
    "        graph1.adj == graph2.adj\n",
    "        and graph1.nodes == graph2.nodes\n",
    "        and graph1.edges == graph2.edges #added\n",
    "        and graph1.graph == graph2.graph \n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = lhs_to_graph(\"a\", true)\n",
    "# expected = _create_graph(['a'], [])\n",
    "# assert(_graphs_equal(expected, res))\n",
    "\n",
    "# res = lhs_to_graph(\"a->b\")\n",
    "# expected = _create_graph(['a','b'], [('a','b')])\n",
    "# assert(_graphs_equal(expected, res))\n",
    "\n",
    "# res = lhs_to_graph(\"a -> b\")\n",
    "# expected = _create_graph(['a','b'], [('a','b')])\n",
    "# assert(_graphs_equal(expected, res))\n",
    "\n",
    "# res = lhs_to_graph(\"a->b -> c\")\n",
    "# expected = _create_graph(['a','b','c'], [('a','b'),('b','c')])\n",
    "# assert(_graphs_equal(expected, res))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = lhs_to_graph(\"a<1,2>[x=5, y: int = 6]\")\n",
    "# expected = _create_graph(['a<1,2>'])\n",
    "# expected.nodes[\"a<1,2>\"][\"x\"] = \"default\"\n",
    "# expected.nodes[\"a<1,2>\"][\"y\"] = \"default\"\n",
    "# assert(_graphs_equal(res,expected))\n",
    "# # print(res.nodes)\n",
    "# # print(res.nodes.data())\n",
    "# # print(res.edges)\n",
    "# # print(\"------\")\n",
    "# # print(expected.nodes)\n",
    "# # print(expected.nodes.data())\n",
    "# # print(expected.edges)\n",
    "\n",
    "# res = lhs_to_graph(\"a[a]->b[ b ] -> c[ c ]\")\n",
    "# expected = _create_graph(['a','b','c'], [('a','b'),('b','c')])\n",
    "# expected.nodes[\"a\"][\"a\"] = \"default\"\n",
    "# expected.nodes[\"b\"][\"b\"] = \"default\"\n",
    "# expected.nodes[\"c\"][\"c\"] = \"default\"\n",
    "# assert(_graphs_equal(expected, res))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiple patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = lhs_to_graph(\"a->b -> c; c-> d\")\n",
    "# expected = _create_graph(['a','b','c','d'], [('a','b'),('b','c'),('c','d')])\n",
    "# assert(_graphs_equal(expected, res))\n",
    "\n",
    "# res = lhs_to_graph(\"a->b -> c; d\")\n",
    "# expected = _create_graph(['a','b','c', 'd'], [('a','b'),('b','c')])\n",
    "# assert(_graphs_equal(expected, res))\n",
    "\n",
    "# res = lhs_to_graph(\"a->b -> c; c[x=5]\")\n",
    "# expected = _create_graph(['a','b','c'], [('a','b'),('b','c')])\n",
    "# expected.nodes[\"c\"][\"x\"] = \"default\"\n",
    "# assert(_graphs_equal(expected, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required_syntax =  \"\"\"\n",
    "# a -> b\n",
    "\n",
    "# a -[x:int = ...]-> b\n",
    "\n",
    "# a -> b[x:int = ...]\n",
    "\n",
    "# a -> b -6+[weight:int]-> c -> d[value:int]\n",
    "# d<0> -> e\n",
    "# d<5> -> e\n",
    "\n",
    "# b -+-> d[value:int]\n",
    "# d<0> -7-> e\n",
    "# e<0,5> -> _\n",
    "\n",
    "# b[ \\\n",
    "# value: str = \\\"hello\\\", \\\n",
    "# id: int \\\n",
    "# ]\n",
    "\n",
    "# b -[\n",
    "# ...\n",
    "# ]-> c \n",
    "\n",
    "# \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
