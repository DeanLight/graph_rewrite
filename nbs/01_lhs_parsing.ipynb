{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| default_exp lhs\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lark\n",
    "# %pip install networkx\n",
    "from lark import Lark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LHS parser ##\n",
    "\n",
    "parsing of the pattern sent as lhs, into a networkX graph representing the template to search.\n",
    "\n",
    "The module converts the declerative constraints regarding the properties of the nodes and edges in the LHS, to imperative functions that are checked together with the 'condition' parameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#attributes: allow optional \\n here, for imperative syntax)\n",
    "#attribute: #[\"=\" value] \n",
    "\n",
    "#    attr_name: /[a-zA-Z0-9]+/ #TODO: lark-imported\n",
    "#    type:  \"int\" | \"string\" | \"bool\" #TODO: escaped string or word\n",
    "#    value: /[0-9a-zA-Z]/\n",
    "\n",
    "#    %import common.WS #CHANGE to allow \\n in the imperative option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# lhs_parser = Lark(r\"\"\"\n",
    "#     %import common.NUMBER -> NATURAL_NUMBER \n",
    "#     %import common.ESCAPED_STRING\n",
    "#     %import common.WS \n",
    "#     %ignore WS\n",
    "\n",
    "#     NAMED_VERTEX: /[a-zA-Z0-9]+/\n",
    "#     ANONYMUS: \"_\"\n",
    "#     ATTR_NAME: /[a-zA-Z0-9]+/\n",
    "#     TYPE:  \"int\" | \"string\" | \"bool\"\n",
    "#     VALUE: /[0-9a-zA-Z]/\n",
    "\n",
    "#     attribute: ATTR_NAME [\":\" TYPE] [\"=\" VALUE]\n",
    "#     attributes: \"[\" attribute (\",\" attribute)* \"]\"\n",
    "\n",
    "#     multi_connection: \"-\" NATURAL_NUMBER \"+\" [attributes] \"->\" \n",
    "#                     | \"-\" NATURAL_NUMBER [attributes] \"->\" \n",
    "#     connection: \"-\" [attributes \"-\"] \">\"\n",
    "#               | multi_connection\n",
    "    \n",
    "#     index_vertex: NAMED_VERTEX \"<\" NATURAL_NUMBER (\",\" NATURAL_NUMBER)* \">\"\n",
    "\n",
    "#     vertex: NAMED_VERTEX [attributes]\n",
    "#         | index_vertex [attributes]\n",
    "#         | ANONYMUS [attributes]\n",
    "\n",
    "#     pattern: vertex (connection vertex)*\n",
    "#     patterns: pattern (\";\" pattern)*\n",
    "        \n",
    "#     \"\"\", parser=\"lalr\", start='patterns' , debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs_parser = Lark(r\"\"\"\n",
    "    %import common.INT -> INT \n",
    "    %import common.FLOAT -> FLOAT\n",
    "    %import common.ESCAPED_STRING -> STRING\n",
    "    %ignore WS\n",
    "\n",
    "    NAMED_VERTEX: /[a-zA-Z0-9]+/\n",
    "    ANONYMUS: \"_\"\n",
    "    ATTR_NAME: /[a-zA-Z0-9]+/\n",
    "    TYPE:  \"int\" | \"string\"\n",
    "    BOOLEAN: \"True\" | \"False\"\n",
    "    NATURAL_NUMBER: /[1-9][0-9]?/\n",
    "    INDEX: /[0-9]+/\n",
    "\n",
    "    value: FLOAT | STRING | INT | BOOLEAN\n",
    "\n",
    "    attribute: ATTR_NAME [\":\" TYPE] [\"=\" value]\n",
    "    attributes: \"[\" attribute (\",\" attribute)* \"]\"\n",
    "\n",
    "    multi_connection: \"-\" NATURAL_NUMBER [attributes] \"->\" \n",
    "    connection: \"-\" [attributes \"-\"] \">\"\n",
    "              | multi_connection\n",
    "    \n",
    "    index_vertex: NAMED_VERTEX \"<\" INDEX (\",\" INDEX)* \">\"\n",
    "\n",
    "    vertex: NAMED_VERTEX [attributes]\n",
    "    | index_vertex [attributes]\n",
    "    | ANONYMUS [attributes]\n",
    "\n",
    "    pattern: vertex (connection vertex)*\n",
    "        \n",
    "    \"\"\", parser=\"lalr\", start='pattern' , debug=True)\n",
    "\n",
    "# multi_connection: \"-\" NATURAL_NUMBER \"+\" [attributes] \"->\"  - setting for the \"-num+->\" feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "The transformer is designed to return the networkX graph representing the patterns.\n",
    "\n",
    "For each branch, the appropriate method will be called with the children of the branch as its argument, and its return value will replace the branch in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import itertools\n",
    "import copy\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "cnt:int = 0 # unique id for anonymous vertices\n",
    "from lark import Tree, Transformer\n",
    "class lhsTransformer(Transformer):\n",
    "    def __init__(self, visit_tokens: bool = True) -> None:\n",
    "        super().__init__(visit_tokens)\n",
    "        self.constraints = {}\n",
    "\n",
    "    def STRING(self, arg):\n",
    "        return arg[1:-1] # remove \" \"\n",
    "    \n",
    "    def BOOLEAN(self, arg):\n",
    "        return bool(arg)\n",
    "    \n",
    "    def INT(self, arg): # can be negative\n",
    "        return int(arg)\n",
    "    \n",
    "    def FLOAT(self, arg):\n",
    "        return float(arg)\n",
    "    \n",
    "    def NATURAL_NUMBER(self, number): # for duplications\n",
    "        return int(number)\n",
    "    \n",
    "    def value(self,arg):\n",
    "        return arg\n",
    "    \n",
    "    def attribute(self, args): #(attr_name, *rest):\n",
    "        attr_name = args[0]\n",
    "        if len(args) == 3:\n",
    "            type, value = args[1:]\n",
    "        elif len(args) == 2:\n",
    "            # CHANGE!\n",
    "            print(\"parsed: \" + args[1])\n",
    "            value = None\n",
    "            type = None\n",
    "        # pass a tuple of attr_name, required type, required value.\n",
    "        return (attr_name, type, value) # constraints are handled in other transformer.\n",
    "    \n",
    "    def attributes(self, attributes): # a list of triples \n",
    "        # return a packed list of the attribute names.\n",
    "        attr_names, constraints = {}, {}\n",
    "        for attribute in attributes:\n",
    "            # print(attribute)\n",
    "            attr_names[str(attribute[0])] = None # will be added to the graph itself\n",
    "            constraints[attribute[0]] = (attribute[1], attribute[2]) # will be added to the conditio function\n",
    "        return (attr_names, constraints)\n",
    "\n",
    "    def multi_connection(self, args): # +\n",
    "        # return the list of attributes(strings), add a special attribute to denote number of duplications.\n",
    "        #   for \"-+->\" implementation also return FALSE if \"+\" is parsed (indicating that the connection is not deterministic)\n",
    "        number, attributes = args\n",
    "        attributes[0][\"$dup\"] = number # removed in graph construction\n",
    "        return attributes\n",
    "\n",
    "    def connection(self, attributes): # (dict of attributes, constraints: attribute -> (val,type) )\n",
    "        # return the packed list of attributes received, num_duplications = 1, is_deterministic = True\n",
    "        attributes[\"$dup\"] = 1\n",
    "        return (attributes, True)\n",
    "\n",
    "        # STOPPED HERE - in pattern, \n",
    "        # during the buiding process, conclude which constraints belong to each edge (for vertices its easier)\n",
    "        # then edit \"condition\" accordingly.\n",
    "\n",
    "    def ANONYMUS(self): #\n",
    "        # return a dedicated name for anonymus (string), and an empty list.\n",
    "        cnt += 1\n",
    "        return (\"$\" + str(cnt), [])\n",
    "\n",
    "    def index_vertex(self, args):\n",
    "        # return the main name of the vertex, and a list of the indices specified.\n",
    "        main_name_tup, *numbers = args #numbers is a list\n",
    "        # print(main_name_tup)\n",
    "        # print(numbers)\n",
    "        return (main_name_tup[0], list(numbers))\n",
    "    \n",
    "    def NAMED_VERTEX(self, name):\n",
    "        # return the main name of the vertex, and an empty list.\n",
    "        return (name, [])\n",
    "\n",
    "    def vertex(self, args): # (vertex_tuple: tuple, attributes: dict = {})\n",
    "        # return arguments\n",
    "        vertex_tuple, *attributes = args # attributes is a empty list/ a list containing a dict\n",
    "        name, indices_list = vertex_tuple \n",
    "        if indices_list == None:\n",
    "            indices_list = []\n",
    "        indices = \",\".join([str(num) for num in indices_list])\n",
    "        new_name = name + \"<\"\n",
    "        new_name = new_name + indices + \">\" # numbers are strings, no convertion needed.\n",
    "        if len(attributes) == 0:\n",
    "            return (new_name, {})\n",
    "        return (new_name, attributes[0])\n",
    "\n",
    "    def pattern(self, args):\n",
    "        # 1) unpack lists of vertices and connections.\n",
    "        vertex, *rest = args\n",
    "        conn, vertices = list(rest)[::2], list(rest)[1::2]\n",
    "        vertices.insert(0,vertex)\n",
    "        print(vertices)\n",
    "        print(conn)\n",
    "        # 2) create a networkX graph:\n",
    "            # if there is a special attribute with TRUE, dumplicate the connection __number__ times.\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # simplified vertion - ignore duplications\n",
    "        G.add_nodes_from(vertices)\n",
    "        edge_list = []\n",
    "        for i,edge in enumerate(conn):\n",
    "            edge_list.append((vertices[i], vertices[i+1], edge[0])) # ignore edge[1] - determinism flag. edge[0] is attributes.\n",
    "\n",
    "        # more complex vertion - duplications\n",
    "        # create a recursive function that adds the vertices and edges, \n",
    "        # that calls itself by the number of duplications on each level.\n",
    "        G.add_edges_from(edge_list)\n",
    "        return G\n",
    "\n",
    "    # def patterns(self, g, *graphs):\n",
    "    #     patterns = list(graphs)\n",
    "    #     patterns.insert(0,g)\n",
    "    #     # unite all the patterns into a single graph\n",
    "    #     G = nx.DiGraph()\n",
    "\n",
    "    #     combined_attributes = dict() # dict of dicts (node_name -> attribute -> value)\n",
    "    #     new_nodes = []\n",
    "    #     new_edges = []\n",
    "    #     for graph in patterns:\n",
    "    #         for node in graph.nodes:\n",
    "    #             combined_attributes[node] = combined_attributes[node] | graph.nodes.data()[node]\n",
    "    #             new_nodes.append(node) #unite the dicts for each\n",
    "    #         for edge in graph.edges:\n",
    "    #             combined_attributes[edge[0]+\",\"+edge[1]] = combined_attributes[edge[0]+\",\"+edge[1]] | graph.edges[edge[0],edge[1]]\n",
    "    #             new_edges.append(edge)\n",
    "\n",
    "    #     G.add_nodes_from([(node, combined_attributes[node]) for node in new_nodes])\n",
    "    #     G.add_edges_from([(node1, node2, combined_attributes[node1+\",\"+node2]) for (node1,node2) in new_edges])\n",
    "    #     return G"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type and constant value checking\n",
    "The transformer is designed to collect the node type and constant node value constraints, such that they are added to the 'condition' parameter to be checked later.\n",
    "\n",
    "This transformer works on a copy of the tree to keep it intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "# lark merge transformers\n",
    "from graph_rewrite.result_set import Match\n",
    "class collectTypeConstraints(Transformer):\n",
    "     \n",
    "    def attribute(self, args): #(attr_name, *rest):\n",
    "        \n",
    "    \n",
    "    def attributes(self, attributes): # a list of tuples \n",
    "        # return a packed list of the attribute names.\n",
    "        # attr_dict = {}\n",
    "        # for attribute in attributes:\n",
    "        #     # print(attribute)\n",
    "        #     attr_dict[str(attribute[0])] = attribute[1]\n",
    "        # return attr_dict\n",
    "\n",
    "    def multi_connection(self, args): # +\n",
    "\n",
    "\n",
    "    def connection(self, attributes): # a dict of attributes\n",
    "        # return the packed list of attributes received, num_duplications = 1, is_deterministic = True\n",
    "        attributes[\"$dup\"] = 1\n",
    "        return (attributes, True)\n",
    "\n",
    "    def ANONYMUS(self): #\n",
    "        # return a dedicated name for anonymus (string), and an empty list.\n",
    "        cnt += 1\n",
    "        return (\"$\" + str(cnt), [])\n",
    "\n",
    "    def index_vertex(self, args):\n",
    "        # return the main name of the vertex, and a list of the indices specified.\n",
    "        main_name_tup, *numbers = args #numbers is a list\n",
    "        # print(main_name_tup)\n",
    "        # print(numbers)\n",
    "        return (main_name_tup[0], list(numbers))\n",
    "    \n",
    "    def NAMED_VERTEX(self, name):\n",
    "        # return the main name of the vertex, and an empty list.\n",
    "        return (name, [])\n",
    "\n",
    "    def vertex(self, args): # (vertex_tuple: tuple, attributes: dict = {})\n",
    "        # return arguments\n",
    "        vertex_tuple, *attributes = args # attributes is a empty list/ a list containing a dict\n",
    "        name, indices_list = vertex_tuple \n",
    "\n",
    "\n",
    "    def pattern(self, args):\n",
    "    # def patterns(self, g, *graphs):\n",
    "\n",
    "\n",
    "\n",
    "    def attribute(self, args):\n",
    "        # return a mapping from attr_name - > required type and value\n",
    "\n",
    "    def attributes(self, args):\n",
    "        return args # return the packed list of the attribute mappings, empty if no attributes.\n",
    "\n",
    "    def vertex(self, args):\n",
    "        name, indices_list, attributes_list = args\n",
    "        # same as lhsTransformer\n",
    "        pass\n",
    "\n",
    "    def pattern(self, vertex, *connections_to_vertex):\n",
    "        # return arguments\n",
    "        pass\n",
    "\n",
    "    def patterns(self, *patterns):\n",
    "        # unpack lists of vertices and connections.\n",
    "        def typeCondition(Match):\n",
    "            # for every vertex in vertex list:\n",
    "                # create full_vertex_name by the attached indices list\n",
    "                # for every attr, type, name required for the vertex:\n",
    "                    # constructor = getName(type) - get the constructor for the type\n",
    "                    # 1) check that the required type and value match together.\n",
    "                    # try:\n",
    "                    #     instance = constructor(value)\n",
    "                    # Except:\n",
    "                        # flag = False: value does not match the type.\n",
    "\n",
    "                    # 2) check that the value constraint holds\n",
    "                    # if getattr(instance, __eq__) == None:\n",
    "                        # flag = False. the type must implement __eq__\n",
    "                    # if not (instance == match[full_vertex_name][attr])\n",
    "                        # flag = false\n",
    "\n",
    "                    # no need to check the type constraint(?), if the value fits. (python)\n",
    "\n",
    "            # TODO: perform the same iterations in the connections list.\n",
    "\n",
    "            #return flag and condition(Match)\n",
    "            pass\n",
    "\n",
    "        return typeCondition #sent as a module output and replaces condition.\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lhs_to_graph(lhs):\n",
    "    tree = lhs_parser.parse(lhs)\n",
    "    final_g = lhsTransformer().transform(tree) #networkx graph\n",
    "    return final_g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grammar Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree = lhs_parser.parse(\"aaaaa->b\") #.pretty(indent_str = \" $ \")\n",
    "# assert(tree != None)\n",
    "# assert(tree)\n",
    "# assert(tree == \n",
    "#     Tree(Token('RULE', 'patterns'),[\n",
    "#       Tree(Token('RULE', 'pattern'),[\n",
    "#         Tree(Token('RULE', 'vertex'),[\n",
    "#           Tree(Token('RULE', 'vertex'),[]),\n",
    "#           None\n",
    "#         ])\n",
    "#       ])\n",
    "#     ]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(nodes, edges=[]):\n",
    "    g = nx.DiGraph()\n",
    "    g.add_nodes_from(nodes)\n",
    "    g.add_edges_from(edges)\n",
    "    return g\n",
    "\n",
    "def graphs_equal(graph1, graph2):  \n",
    "    return (\n",
    "        graph1.adj == graph2.adj\n",
    "        and graph1.nodes == graph2.nodes\n",
    "        and graph1.edges == graph2.edges #added\n",
    "        and graph1.graph == graph2.graph \n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = lhs_to_graph(\"a\")\n",
    "expected = create_graph(['a'], [])\n",
    "assert(graphs_equal(expected, res))\n",
    "\n",
    "res = lhs_to_graph(\"a->b\")\n",
    "expected = create_graph(['a','b'], [('a','b')])\n",
    "assert(graphs_equal(expected, res))\n",
    "\n",
    "res = lhs_to_graph(\"a -> b\")\n",
    "expected = create_graph(['a','b'], [('a','b')])\n",
    "assert(graphs_equal(expected, res))\n",
    "\n",
    "res = lhs_to_graph(\"a->b -> c\")\n",
    "expected = create_graph(['a','b','c'], [('a','b'),('b','c')])\n",
    "assert(graphs_equal(expected, res))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = lhs_to_graph(\"a<1,2>[x=5, y: int = 6]\")\n",
    "expected = create_graph(['a<1,2>'])\n",
    "expected.nodes[\"a<1,2>\"][\"x\"] = \"default\"\n",
    "expected.nodes[\"a<1,2>\"][\"y\"] = \"default\"\n",
    "assert(graphs_equal(res,expected))\n",
    "# print(res.nodes)\n",
    "# print(res.nodes.data())\n",
    "# print(res.edges)\n",
    "# print(\"------\")\n",
    "# print(expected.nodes)\n",
    "# print(expected.nodes.data())\n",
    "# print(expected.edges)\n",
    "\n",
    "res = lhs_to_graph(\"a[a]->b[ b ] -> c[ c ]\")\n",
    "expected = create_graph(['a','b','c'], [('a','b'),('b','c')])\n",
    "expected.nodes[\"a\"][\"a\"] = \"default\"\n",
    "expected.nodes[\"b\"][\"b\"] = \"default\"\n",
    "expected.nodes[\"c\"][\"c\"] = \"default\"\n",
    "assert(graphs_equal(expected, res))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiple patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = lhs_to_graph(\"a->b -> c; c-> d\")\n",
    "expected = create_graph(['a','b','c','d'], [('a','b'),('b','c'),('c','d')])\n",
    "assert(graphs_equal(expected, res))\n",
    "\n",
    "res = lhs_to_graph(\"a->b -> c; d\")\n",
    "expected = create_graph(['a','b','c', 'd'], [('a','b'),('b','c')])\n",
    "assert(graphs_equal(expected, res))\n",
    "\n",
    "res = lhs_to_graph(\"a->b -> c; c[x=5]\")\n",
    "expected = create_graph(['a','b','c'], [('a','b'),('b','c')])\n",
    "expected.nodes[\"c\"][\"x\"] = \"default\"\n",
    "assert(graphs_equal(expected, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required_syntax =  \"\"\"\n",
    "# a -> b\n",
    "\n",
    "# a -[x:int = ...]-> b\n",
    "\n",
    "# a -> b[x:int = ...]\n",
    "\n",
    "# a -> b -6+[weight:int]-> c -> d[value:int]\n",
    "# d<0> -> e\n",
    "# d<5> -> e\n",
    "\n",
    "# b -+-> d[value:int]\n",
    "# d<0> -7-> e\n",
    "# e<0,5> -> _\n",
    "\n",
    "# b[ \\\n",
    "# value: str = \\\"hello\\\", \\\n",
    "# id: int \\\n",
    "# ]\n",
    "\n",
    "# b -[\n",
    "# ...\n",
    "# ]-> c \n",
    "\n",
    "# \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
