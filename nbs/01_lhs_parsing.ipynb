{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LHS Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import show_doc\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "This module defines the grammar of the LHS that is given by the user to the *rewrite* function of the library.\n",
    "The module is also responsible for parsing of the pattern sent as LHS, into a networkX graph representing the template to search.\n",
    "\n",
    "The module converts the declerative constraints regarding the properties of the nodes and edges in the LHS, to imperative functions that are checked together with the 'condition' parameter of *rewrite*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lark import Lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lark.lark.Lark"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import copy\n",
    "from collections.abc import Callable\n",
    "import networkx as nx\n",
    "from lark import Transformer, Lark\n",
    "from lark import UnexpectedCharacters, UnexpectedToken\n",
    "from graph_rewrite.match_class import Match\n",
    "from graph_rewrite.core import GraphRewriteException\n",
    "from graph_rewrite.core import _create_graph,  _graphs_equal, draw\n",
    "from collections import defaultdict\n",
    "from graph_rewrite.match_class import _convert_to_edge_name\n",
    "from typing import Tuple, Union"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar\n",
    "The grammar induces the allowed syntax of a legal LHS string that can be provided by the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "lhs_parser = Lark(r\"\"\"\n",
    "    %import common.INT -> INT \n",
    "    %import common.FLOAT -> FLOAT\n",
    "    %import common.ESCAPED_STRING -> STRING\n",
    "    %import common.WS -> WS\n",
    "    %ignore WS\n",
    "\n",
    "    NAMED_VERTEX: /[_a-zA-Z0-9]+/\n",
    "    ANONYMUS: \"_\"\n",
    "    ATTR_NAME: /[_a-zA-Z0-9]+/\n",
    "    TYPE:  \"int\" | \"str\" | \"bool\" | \"float\"\n",
    "    BOOLEAN: \"True\" | \"False\"\n",
    "    NATURAL_NUMBER: /[1-9][0-9]*/\n",
    "    INDEX: /[0-9]+/\n",
    "\n",
    "    value: FLOAT | INT | BOOLEAN | STRING\n",
    "\n",
    "    attribute: ATTR_NAME [\":\" TYPE] [\"=\" value]\n",
    "    attributes: \"[\" attribute (\",\" attribute)* \"]\"\n",
    "\n",
    "    multi_connection: \"-\" NATURAL_NUMBER [attributes] \"->\" \n",
    "    connection: [\"-\" attributes]\"->\"\n",
    "              | multi_connection\n",
    "    \n",
    "    index_vertex: NAMED_VERTEX \"<\" INDEX (\",\" INDEX)* \">\"\n",
    "\n",
    "    vertex: NAMED_VERTEX [attributes]\n",
    "    | index_vertex [attributes]\n",
    "    | ANONYMUS [attributes]\n",
    "\n",
    "    pattern: vertex (connection vertex)*\n",
    "    patterns: pattern (\",\" pattern)* \n",
    "    lhs: patterns [\";\" patterns]\n",
    "\n",
    "    \"\"\", parser=\"lalr\", start='lhs', debug=True)\n",
    "\n",
    "\n",
    "# multi_connection: \"-\" NATURAL_NUMBER \"+\" [attributes] \"->\"  - setting for the \"-num+->\" feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "The transformer is designed to return the networkX graph representing the patterns received by the user.\n",
    "\n",
    "For each branch, the appropriate method will be called with the children of the branch as its argument, and its return value will replace the matching node in the tree.\n",
    "\n",
    "The secondary task of the transformer is to collect the node/edge type and constant node/edge value constraints, such that they are added to the 'condition' parameter to be checked later. Thus, the lhsTransformer contains a python dictionary *constraints* which accumulates the constraints from all components of the parsed graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "RenderFunc = Callable[[Match], any] # type of a function to render a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "cnt:int = 0 # unique id for anonymous vertices\n",
    "class graphRewriteTransformer(Transformer):\n",
    "    def __init__(self, visit_tokens: bool = True, component: str = \"LHS\", match: Match = None, render_funcs: dict[str, RenderFunc] = {}) -> None:\n",
    "        super().__init__(visit_tokens)\n",
    "        # general\n",
    "        self.component = component\n",
    "        # RHS parameters\n",
    "        self.match = match\n",
    "        self.render_funcs = render_funcs\n",
    "        # LHS parameters\n",
    "        self.constraints = {}\n",
    "        self.cnt = 0\n",
    "\n",
    "    def STRING(self, arg):\n",
    "        # remove \" \"\n",
    "        return arg[1:-1] \n",
    "    \n",
    "    def BOOLEAN(self, arg):\n",
    "        return bool(arg)\n",
    "    \n",
    "    def INT(self, arg):\n",
    "        # can be negative\n",
    "        return int(arg)\n",
    "    \n",
    "    def FLOAT(self, arg):\n",
    "        return float(arg)\n",
    "    \n",
    "    def NATURAL_NUMBER(self, number): \n",
    "        # represents number of duplications\n",
    "        return int(number)\n",
    "    \n",
    "    def USER_VALUE(self, arg):\n",
    "        # get the variable name\n",
    "        variable = arg[2:-2]\n",
    "        # extract the actual value supplied by the user - can be of any type.\n",
    "        return self.render_funcs[variable](self.match) \n",
    "    \n",
    "    def value(self, args): \n",
    "        # one argument encased in a list\n",
    "        return args[0]\n",
    "    \n",
    "    def attribute(self, args): \n",
    "        # if an optional token was not parsed, None is placed in the parse tree.\n",
    "        # if type and value are not allowed, then None is entered manualy.\n",
    "        if self.component == \"P\": \n",
    "            attr_name = args[0]\n",
    "            type, value = None, None\n",
    "        else:\n",
    "            attr_name, type, value = args       \n",
    "        \n",
    "        if type is not None and type not in [\"int\", \"str\", \"bool\", \"float\"]:\n",
    "            raise GraphRewriteException(f\"Type '{type}' is not one of the types supported by the LHS parser: int, str, bool, float or None. If another type is needed, please use the condition function.\")\n",
    "            \n",
    "        return (attr_name, type, value)\n",
    "    \n",
    "    def attributes(self, attributes): # a list of triples \n",
    "        # return a packed list of the attribute names.\n",
    "        attr_names, constraints = {}, {}\n",
    "        for attribute in attributes:\n",
    "            # will be added to the graph itself\n",
    "            attr_name, type, value = attribute\n",
    "            if self.component == \"LHS\":\n",
    "                attr_names[str(attr_name)] = (None, None)\n",
    "                # will be added to the condition function\n",
    "                constraints[str(attr_name)] = (type, value) \n",
    "            else:\n",
    "                attr_names[str(attr_name)] = (type, value)\n",
    "\n",
    "        return (attr_names, constraints)\n",
    "\n",
    "    def multi_connection(self, args): # +\n",
    "        # return the list of attributes(strings), add a special attribute to denote number of duplications.\n",
    "        number, attributes = args\n",
    "        if attributes == None:\n",
    "            attributes = ({},{})\n",
    "        # add a special atrribute to handle duplications during construction\n",
    "        attributes[0][\"$dup\"] = number \n",
    "        return attributes\n",
    "\n",
    "    def connection(self, args): \n",
    "        # (tuple of dicts: attributes, constraints. attributes is of the form: attribute -> val)\n",
    "        attributes = args[0]\n",
    "        if attributes == None:\n",
    "            attributes = ({},{})\n",
    "        # add a special atrribute to handle duplications during construction\n",
    "        attributes[0][\"$dup\"] = 1\n",
    "        return (attributes, True)\n",
    "\n",
    "    def ANONYMUS(self, _): #\n",
    "        # return a dedicated name for anonymus (string), and an empty indices list.\n",
    "        x = \"_anonymous_node_\" + str(self.cnt)\n",
    "        self.cnt += 1\n",
    "        return (x, [])\n",
    "\n",
    "    def index_vertex(self, args):\n",
    "        # return the main name of the vertex, and a list of the indices specified.\n",
    "        main_name_tup, *numbers = args #numbers is a list\n",
    "        return (main_name_tup[0], list(numbers))\n",
    "    \n",
    "    def NAMED_VERTEX(self, name):\n",
    "        if name.startswith(\"_anonymous_node_\"):\n",
    "            raise GraphRewriteException(\"_anonymous_node_ prefix cannot be used for a vertex name in LHS\")\n",
    "        return (str(name), [])\n",
    "\n",
    "    def vertex(self, args): # (vertex_tuple: tuple, attributes: list)\n",
    "        # attributes is a empty list/ a list containing a tuple: (names dict, constraints dict)\n",
    "        vertex_tuple, *attributes = args \n",
    "        name, indices_list = vertex_tuple\n",
    "\n",
    "        # create new name\n",
    "        indices = \",\".join([str(num) for num in indices_list])\n",
    "        if len(indices) == 0:\n",
    "            new_name = str(name)\n",
    "        else:\n",
    "            new_name =  name + \"<\" + indices + \">\" \n",
    "\n",
    "        # no attributes to handle\n",
    "        if attributes[0] == None:\n",
    "            return (new_name, {})\n",
    "        \n",
    "        # now that we have the vertex name we add the attribute constraints:\n",
    "        # vertices may appear multiple times in LHS thus we unite the constraints. We assume there cannot be contradicting constraints.\n",
    "        attribute_names, constraints = attributes[0] \n",
    "        # the second element of the tuple is the constraints dict: attr_name -> (value,type)\n",
    "        if self.component == \"LHS\":\n",
    "            if new_name not in self.constraints.keys():\n",
    "                self.constraints[new_name] = {}\n",
    "            self.constraints[new_name] = self.constraints[new_name] | constraints \n",
    "        return (new_name, attribute_names)\n",
    "\n",
    "    def pattern(self, args):\n",
    "        # 1) unpack lists of vertices and connections.\n",
    "        vertex, *rest = args\n",
    "        conn, vertices = list(rest)[::2], list(rest)[1::2]\n",
    "        vertices.insert(0,vertex)\n",
    "        # 2) create a networkX graph:\n",
    "            # Future feature: if there is a special attribute with TRUE (deterministic), dumplicate the connection $dup times.\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # simplified vertion - ignore duplications\n",
    "        G.add_nodes_from(vertices)\n",
    "        edge_list = []\n",
    "        for i,edge in enumerate(conn):\n",
    "            # for now the duplication feature is not included so we remove the $dup attribute\n",
    "            # we handeled None in the connection rule.\n",
    "            attribute_names, constraints = edge[0]\n",
    "            attribute_names.pop(\"$dup\", 0)\n",
    "            # ignore edge[1] - determinism flag. edge[0] is the tuple of dicts of attributes.\n",
    "            vertex_name_pos = 0 # each item in vertices is a tuple (vertex_name, attrs)\n",
    "            edge_list.append((vertices[i][vertex_name_pos], vertices[i+1][vertex_name_pos], attribute_names)) \n",
    "\n",
    "            # add constraints - we assume an edge only appears once in LHS\n",
    "            if self.component == \"LHS\":\n",
    "                filtered_cons = dict(filter(lambda tup: not tup[1] == (None, None), constraints.items()))\n",
    "                # check if filtered_cons is not empty - there are concrete constraints\n",
    "                if filtered_cons: \n",
    "                    self.constraints[str(vertices[i][vertex_name_pos]) + \"->\" + str(vertices[i+1][vertex_name_pos])] = filtered_cons\n",
    "\n",
    "        # more complex vertion - duplications\n",
    "        # create a recursive function that adds the vertices and edges, \n",
    "        # that calls itself by the number of duplications on each level.\n",
    "\n",
    "        G.add_edges_from(edge_list)\n",
    "        return G\n",
    "\n",
    "    def empty(self, _):\n",
    "        return nx.DiGraph()\n",
    "    \n",
    "    def patterns(self, args):\n",
    "        g, *graphs = args\n",
    "        graphs.insert(0,g)\n",
    "        # unite all the patterns into a single graph\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # dict of dicts (node_name -> attribute -> None/someValue)\n",
    "        combined_attributes = dict() \n",
    "        new_nodes = []\n",
    "        new_edges = []\n",
    "        for graph in graphs:\n",
    "            for node in graph.nodes:\n",
    "                if node not in combined_attributes.keys():\n",
    "                    combined_attributes[node] = {}\n",
    "                combined_attributes[node] = combined_attributes[node] | graph.nodes.data()[node]\n",
    "                #unite the dicts for each\n",
    "                new_nodes.append(node) \n",
    "            for edge in graph.edges:\n",
    "                # we assumed edges cannot appear more than once in LHS\n",
    "                combined_attributes[edge[0] + \"->\" + edge[1]] = graph.edges[edge[0],edge[1]]\n",
    "                new_edges.append(edge)\n",
    "        # filtered_attr = dict(filter(lambda _,value: not value == (None, None), combined_attributes.items()))\n",
    "        G.add_nodes_from([(node, combined_attributes[node]) for node in new_nodes])\n",
    "        G.add_edges_from([(node1, node2, combined_attributes[node1 + \"->\" + node2]) for (node1,node2) in new_edges])\n",
    "        \n",
    "        #sent as a module output and replaces condition.\n",
    "        constraints = copy.deepcopy(self.constraints)\n",
    "        self.constraints = {}\n",
    "        return (G, constraints) \n",
    "\n",
    "    def lhs(self, args):\n",
    "        return [arg for arg in args if arg is not None]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Application\n",
    "The following function applies the transformer on an LHS-formatted string provided by the user, to extract the constraints and the resulting networkx greaph. Then it unites the constraints with the constraints given in the *condition* function supplied by the user, so that they will be inforced together later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lhs_to_graph(lhs: str, debug: bool = False) -> Tuple[nx.DiGraph, nx.DiGraph]:\n",
    "    \"\"\"\n",
    "    Converts a LHS string to a networkx graph and extracts constraints.\n",
    "\n",
    "    Args:\n",
    "    - lhs: str - a string representing the LHS of a rule.\n",
    "    - debug: bool - if True, returns the parse tree and the collections tree, instead of the graphs.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple[nx.DiGraph, nx.DiGraph] - a tuple of two networkx graphs: the single nodes graph and the collections graph.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parse_tree = lhs_parser.parse(lhs)\n",
    "        if debug:\n",
    "            return parse_tree, None\n",
    "\n",
    "        transformer = graphRewriteTransformer(component=\"LHS\")\n",
    "        patterns_list = transformer.transform(parse_tree)  # List of (graph, constraints)\n",
    "\n",
    "        if len(patterns_list) == 1:\n",
    "            single_nodes_graph, single_nodes_constraints = patterns_list[0]\n",
    "            collections_graph = nx.DiGraph()\n",
    "            collections_constraints = {}\n",
    "        elif len(patterns_list) == 2:\n",
    "            single_nodes_graph, single_nodes_constraints = patterns_list[0]\n",
    "            collections_graph, collections_constraints = patterns_list[1]\n",
    "        else:\n",
    "            raise GraphRewriteException(\"Unexpected number of pattern sets in LHS.\")\n",
    "\n",
    "        _add_constraints_to_graph(single_nodes_graph, single_nodes_constraints)\n",
    "        _add_constraints_to_graph(collections_graph, collections_constraints)\n",
    "\n",
    "        return single_nodes_graph, collections_graph\n",
    "\n",
    "    except (BaseException, UnexpectedCharacters, UnexpectedToken) as e:\n",
    "        raise GraphRewriteException('Unable to convert LHS: {}'.format(e))\n",
    "\n",
    "\n",
    "def _add_constraints_to_graph(graph: nx.DiGraph, constraints: dict):\n",
    "    \"\"\"\n",
    "    Adds constraints to a graph, by going over the constraints dict and adding them to the graph, \n",
    "    such that each node or edge has the a dictionary of constraints - attr_name -> (attr_type_str, attr_value).\n",
    "\n",
    "    Args:\n",
    "    - graph: nx.DiGraph - the graph to add constraints to.\n",
    "    - constraints: dict - the constraints to add to the graph.\n",
    "    \"\"\"\n",
    "\n",
    "    for graph_obj, attr_constraints in constraints.items():\n",
    "        for attr_name in attr_constraints.keys():\n",
    "            if graph_obj in graph.nodes:\n",
    "                graph.nodes[graph_obj][attr_name] = attr_constraints[attr_name]\n",
    "            else: \n",
    "                node1, node2 = graph_obj.split(\"->\")\n",
    "                graph.edges[node1, node2][attr_name] = attr_constraints[attr_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, _ = lhs_to_graph(\"a\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests\n",
    "Note that throughout these tests, we use the naive condition which returns True for all matches. We chose to do that since this module is all about parsing, which is not affected by the condition.\n",
    "The condition will be checked appropriately in the module that actually uses it, the Matcher module."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, _ = lhs_to_graph(\"a\")\n",
    "expected = _create_graph(['a'], [])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _ = lhs_to_graph(\"a->b\")\n",
    "expected = _create_graph(['a','b'], [('a','b')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _ = lhs_to_graph(\"a -> b\")\n",
    "expected = _create_graph(['a','b'], [('a','b')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _ = lhs_to_graph(\"a->b -> c\")\n",
    "expected = _create_graph(['a','b','c'], [('a','b'),('b','c')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _ = lhs_to_graph(\"a->b -> a\")\n",
    "expected = _create_graph(['a','b'], [('a','b'),('b','a')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "# anonymus vertices\n",
    "res, _ = lhs_to_graph(\"a->_->b->_\")\n",
    "expected = _create_graph(['a','b','_anonymous_node_0','_anonymous_node_1'], [('a','_anonymous_node_0'),('_anonymous_node_0','b'),('b','_anonymous_node_1')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, _ = lhs_to_graph(\"a[x=1]->b\")\n",
    "expected = _create_graph([('a', {'x': (None, 1)}), ('b', {})], [('a', 'b', {})])\n",
    "assert(_graphs_equal(expected, res))\n",
    "\n",
    "res, _ = lhs_to_graph(\"a[x]->b\")\n",
    "expected = _create_graph([('a', {'x': (None, None)}), ('b', {})], [('a', 'b', {})])\n",
    "assert(_graphs_equal(expected, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmZsb3djaGFydCBUQgphWyJhCiJdCmJbImIKIl0KYSAtLT58Ing9KE5vbmUsIE5vbmUpInwgYgo=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res, _ = lhs_to_graph(\"a-[x=1]->b\")\n",
    "expected = _create_graph([('a', {}), ('b', {})], [('a', 'b', {'x': (None, 1)})])\n",
    "assert(_graphs_equal(expected, res))\n",
    "\n",
    "res, _ = lhs_to_graph(\"a-[x]->b\")\n",
    "expected = _create_graph([('a', {}), ('b', {})], [('a', 'b', {'x': (None, None)})])\n",
    "assert(_graphs_equal(expected, res))\n",
    "\n",
    "draw(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, _ = lhs_to_graph(\"a[x=5]\")\n",
    "expected = _create_graph([('a', {'x': (None, 5)})], [])\n",
    "assert(_graphs_equal(expected, res))\n",
    "\n",
    "res, _ = lhs_to_graph(\"a-[x=5]->b\")\n",
    "expected = _create_graph([('a', {}), ('b', {})], [('a', 'b', {'x': (None, 5)})])\n",
    "assert(_graphs_equal(expected, res))\n",
    "\n",
    "res, _ = lhs_to_graph(\"a<1,2>[x=5, y: int = 6]\")\n",
    "expected = _create_graph([('a<1,2>', {'x': (None, 5), 'y': ('int', 6)})], [])\n",
    "assert(_graphs_equal(expected, res))\n",
    "\n",
    "res, _ = lhs_to_graph(\"a[a]-[x]->b[ b ] -> c[ c ]\")\n",
    "expected = _create_graph([('a', {'a': (None, None)}), ('b', {'b': (None, None)}), ('c', {'c': (None, None)})],[])\n",
    "assert(_graphs_equal(expected, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmZsb3djaGFydCBUQgpyZWxbInJlbAp2YWw9KFRva2VuKCNxdW90O1RZUEUjcXVvdDssICNxdW90O3N0ciNxdW90OyksICNxdW90O3JlbGF0aW9uI3F1b3Q7KSJdCnpbInoKdmFsPShUb2tlbigjcXVvdDtUWVBFI3F1b3Q7LCAjcXVvdDtzdHIjcXVvdDspLCAjcXVvdDtyZWxhdGlvbl9uYW1lI3F1b3Q7KSJdCnJlbCAtLT4gego=\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Tree(Token('RULE', 'lhs'), [Tree(Token('RULE', 'patterns'), [Tree(Token('RULE', 'pattern'), [Tree(Token('RULE', 'vertex'), [Token('NAMED_VERTEX', 'rel'), Tree(Token('RULE', 'attributes'), [Tree(Token('RULE', 'attribute'), [Token('ATTR_NAME', 'val'), Token('TYPE', 'str'), Tree(Token('RULE', 'value'), [Token('STRING', '\"relation\"')])])])]), Tree(Token('RULE', 'connection'), [None]), Tree(Token('RULE', 'vertex'), [Token('NAMED_VERTEX', 'z'), Tree(Token('RULE', 'attributes'), [Tree(Token('RULE', 'attribute'), [Token('ATTR_NAME', 'val'), Token('TYPE', 'str'), Tree(Token('RULE', 'value'), [Token('STRING', '\"relation_name\"')])])])])])]), None]), None)\n"
     ]
    }
   ],
   "source": [
    "t2 = lhs_to_graph('''rel[val:str=\"relation\"]->z[val:str=\"relation_name\"]''',True)\n",
    "g2, _ = lhs_to_graph('''rel[val:str=\"relation\"]->z[val:str=\"relation_name\"]''' )\n",
    "draw(g2)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String Type Attributes ###\n",
    "Testing the parser's ability to handle attributes of type string, specifically when they are explicitly provided in the LHS string using formats such as \"<value>\" or other string representation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a non string value to have a basic test\n",
    "res, _ = lhs_to_graph(\"A[attrA=1]->B\")\n",
    "expected = _create_graph([('A', {'attrA': (None, 1)}), 'B'], [('A', 'B', {})])\n",
    "assert(_graphs_equal(expected, res))\n",
    "\n",
    "# Using a string value\n",
    "res, _ = lhs_to_graph(\"A[attrA=\\\"1\\\"]->B\")\n",
    "expected = _create_graph([('A', {'attrA': (None, '1')}), 'B'], [('A', 'B', {})])\n",
    "assert(_graphs_equal(expected, res))\n",
    "\n",
    "lhs, _ = lhs_to_graph('''rel[val=\"relation\"]->z[val=\"relation_name\"]''')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multiple patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, _ = lhs_to_graph(\"a->b -> c, c-> d\") \n",
    "expected = _create_graph(['a','b','c','d'], [('a','b'),('b','c'),('c','d')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _ = lhs_to_graph(\"a->b -> c, d\") \n",
    "expected = _create_graph(['a','b','c', 'd'], [('a','b'),('b','c')])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)\n",
    "\n",
    "res, _ = lhs_to_graph(\"a->b[z] -> c[y], c[x=5]->b[r]\") \n",
    "expected = _create_graph([('a', {}), ('b', {'z': (None, None), 'r': (None, None)}), ('c', {'y': (None, None), 'x': (None, 5)})],[])\n",
    "assert(_graphs_equal(expected, res))\n",
    "#_plot_graph(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('c', {'type': (None, 'course')})]\n",
      "[('s', {'type': (None, 'student')}), ('c', {})]\n"
     ]
    }
   ],
   "source": [
    "res_single, res_collection = lhs_to_graph('c[type=\"course\"];s[type=\"student\"]->c') \n",
    "print(res_single.nodes.data())\n",
    "print(res_collection.nodes.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('c', {'type': (None, 'course')}), ('_anonymous_node_0', {})]\n"
     ]
    }
   ],
   "source": [
    "res_single, _ = lhs_to_graph('c[type=\"course\"]->_;s[type=\"student\"]->c')\n",
    "print(res_single.nodes.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to convert LHS: Error trying to process rule \"NAMED_VERTEX\":\n",
      "\n",
      "_anonymous_node_ prefix cannot be used for a vertex name in LHS\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    res_single, _ = lhs_to_graph('c[type=\"course\"]->_anonymous_node_0')\n",
    "except GraphRewriteException as e:\n",
    "    # Should raise an exception: Unable to convert LHS: Error trying to process rule \"NAMED_VERTEX\": _anonymous_node_ prefix cannot be used for a vertex name in LHS\n",
    "   print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collections (two graph, \";\" delimiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to convert LHS: Unexpected token Token('$END', '') at line 1, column 17.\n",
      "Expected one of: \n",
      "\t* NAMED_VERTEX\n",
      "\t* ANONYMUS\n",
      "\n",
      "Unable to convert LHS: Unexpected token Token('SEMICOLON', ';') at line 1, column 1.\n",
      "Expected one of: \n",
      "\t* NAMED_VERTEX\n",
      "\t* ANONYMUS\n",
      "Previous tokens: [None]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res1, res2 = lhs_to_graph(\"a->b -> c, c-> d; a->b -> c, d\") \n",
    "expected1 = _create_graph(['a','b','c','d'], [('a','b'),('b','c'),('c','d')])\n",
    "expected2 = _create_graph(['a','b','c', 'd'], [('a','b'),('b','c')])\n",
    "assert(_graphs_equal(expected1, res1))\n",
    "assert(_graphs_equal(expected2, res2))\n",
    "\n",
    "try:\n",
    "    lhs_to_graph(\"a->b -> c, c-> d;\")\n",
    "except GraphRewriteException as e:\n",
    "    # Should raise an exception: Unable to convert LHS: Unexpected token Token('$END', ''), Expected one of: \n",
    "    # * ANONYMUS\n",
    "\t# * NAMED_VERTEX\n",
    "    print(e)\n",
    "    \n",
    "try:\n",
    "    lhs_to_graph(\";a->b -> c, c-> d\")\n",
    "except GraphRewriteException as e:\n",
    "    # Should raise an exception: Unable to convert LHS: Unexpected token Token('$END', ''), Expected one of: \n",
    "    # * ANONYMUS\n",
    "\t# * NAMED_VERTEX\n",
    "    print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1, res2 = lhs_to_graph('x->y[attr];x->z')\n",
    "expected1 = _create_graph([('x'), ('y', {'attr': (None, None)})], [('x', 'y')])\n",
    "expected2 = _create_graph([('x'), ('z')], [('x', 'z')])\n",
    "assert _graphs_equal(expected1, res1)\n",
    "assert _graphs_equal(expected2, res2)\n",
    "\n",
    "res1, res2 = lhs_to_graph('x->y;x->z[attr]')\n",
    "expected1 = _create_graph([('x'), ('y')], [('x', 'y')])\n",
    "expected2 = _create_graph([('x'), ('z', {'attr': (None, None)})], [('x', 'z')])\n",
    "assert _graphs_equal(expected1, res1)\n",
    "assert _graphs_equal(expected2, res2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|hide\n",
    "import nbdev; nbdev.nbdev_export()\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
