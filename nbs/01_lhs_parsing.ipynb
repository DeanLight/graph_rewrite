{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| default_exp lhs\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install lark\n",
    "# %pip install networkx\n",
    "from lark import Lark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LHS parser ##\n",
    "\n",
    "parsing of the pattern sent as lhs, into a networkX graph representing the template to search.\n",
    "\n",
    "The module converts the declerative constraints regarding the properties of the nodes and edges in the LHS, to imperative functions that are checked together with the 'condition' parameter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#attributes: allow optional \\n here, for imperative syntax)\n",
    "#attribute: #[\"=\" value] \n",
    "\n",
    "#    attr_name: /[a-zA-Z0-9]+/ #TODO: lark-imported\n",
    "#    type:  \"int\" | \"string\" | \"bool\" #TODO: escaped string or word\n",
    "#    value: /[0-9a-zA-Z]/\n",
    "\n",
    "#    %import common.WS #CHANGE to allow \\n in the imperative option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "lhs_parser = Lark(r\"\"\"\n",
    "    %import common.NUMBER -> NATURAL_NUMBER \n",
    "    %import common.ESCAPED_STRING\n",
    "    %import common.WS \n",
    "    %ignore WS\n",
    "\n",
    "    NAMED_VERTEX: /[a-zA-Z0-9]+/ \n",
    "    ANONYMUS: \"_\"\n",
    "    ATTR_NAME: /[a-zA-Z0-9]+/ \n",
    "    TYPE:  \"int\" | \"string\" | \"bool\" \n",
    "    VALUE: /[0-9a-zA-Z]/\n",
    "\n",
    "    attribute: ATTR_NAME [\":\" TYPE] [\"=\" VALUE] \n",
    "    attributes: \"\\[\" attribute (\",\" attribute)* \"\\]\"\n",
    "\n",
    "    multi_connection: \"-\" NATURAL_NUMBER \"+\" [attributes] \"->\" \n",
    "                    | \"-\" NATURAL_NUMBER [attributes] \"->\" \n",
    "    connection: \"-\" [attributes \"-\"] \">\"\n",
    "              | multi_connection\n",
    "    \n",
    "    index_vertex: NAMED_VERTEX \"<\" NATURAL_NUMBER (\",\" NATURAL_NUMBER)* \">\"\n",
    "\n",
    "    vertex: NAMED_VERTEX [attributes]\n",
    "        | index_vertex [attributes]\n",
    "        | ANONYMUS [attributes]\n",
    "\n",
    "    pattern: vertex (connection vertex)*\n",
    "    patterns: pattern (\";\" pattern)*\n",
    "        \n",
    "    \"\"\", parser=\"lalr\", start='patterns' , debug=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "The transformer is designed to return the networkX graph representing the patterns.\n",
    "\n",
    "For each branch, the appropriate method will be called with the children of the branch as its argument, and its return value will replace the branch in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import itertools\n",
    "import copy\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "cnt:int = 0 # unique id for anonymous vertices\n",
    "from lark import Tree, Transformer\n",
    "class lhsTransformer(Transformer):\n",
    "    def NATURAL_NUMBER(self, number):\n",
    "        return int(number)\n",
    "    \n",
    "    def attribute(self, attr_name, _):\n",
    "        return (attr_name, \"default\") # constraints are handled in other transformer.\n",
    "    \n",
    "    def attributes(self, *attributes):\n",
    "        # return a packed list of the attribute names.\n",
    "        attr_dict = {}\n",
    "        for attribute in list(attributes):\n",
    "            attr_dict[attribute[0]] = attribute[1]\n",
    "        return attr_dict\n",
    "\n",
    "    def multi_connection(self, number, attributes: dict): # +\n",
    "        # renewed: return the list of attributes(strings), add a special attribute to denote number of duplications,\n",
    "        #   and FALSE (indicating that the connection is not deterministic)\n",
    "        attributes[\"$dup\"] = number\n",
    "        return (attributes, False)\n",
    "    \n",
    "    def multi_connection(self, number, attributes: dict): # no +\n",
    "        # renewed: return the list of attributes(strings), add a special attribute to denote number of duplications,\n",
    "        #   and TRUE (indicating that the connection is not deterministic)\n",
    "        attributes[\"$dup\"] = number\n",
    "        return (attributes, True)\n",
    "\n",
    "    def connection(self, multiconnection_params: tuple): #multiconnection\n",
    "        # return the packed list of attributes received, num_duplications, is_deterministic\n",
    "        return multiconnection_params\n",
    "\n",
    "    def connection(self, attributes): # a dict of attributes\n",
    "        # return the packed list of attributes received, num_duplications = 1, is_deterministic = True\n",
    "        attributes[\"$dup\"] = 1\n",
    "        return (attributes, True)\n",
    "\n",
    "    def ANONYMUS(self): #\n",
    "        # return a dedicated name for anonymus (string), and an empty list.\n",
    "        cnt += 1\n",
    "        return (\"$\" + str(cnt), [])\n",
    "\n",
    "    def index_vertex(self, main_name, *numbers):\n",
    "        # return the main name of the vertex, and a list of the indices specified.\n",
    "        return (main_name, list(numbers))\n",
    "    \n",
    "    def NAMED_VERTEX(self, name):\n",
    "        # return the main name of the vertex, and an empty list.\n",
    "        return (name, [])\n",
    "\n",
    "    def vertex(self, vertex_tuple: tuple, attributes: dict = {}):\n",
    "        # return arguments\n",
    "        name, indices_list = vertex_tuple\n",
    "        print(name)\n",
    "        if indices_list == None:\n",
    "            indices_list = []\n",
    "        indices = \",\".join([str(num) for num in indices_list])\n",
    "        new_name = name + \"<\"\n",
    "        new_name = new_name + indices + \">\" # numbers are strings, no convertion needed.\n",
    "        return (new_name, attributes)\n",
    "\n",
    "    def pattern(self, vertex, *rest):\n",
    "        # 1) unpack lists of vertices and connections.\n",
    "        conn, vertices = list(rest)[::2], list(rest)[1::2]\n",
    "        vertices.insert(0,vertex)\n",
    "        # 2) create a networkX graph:\n",
    "            # if there is a special attribute with TRUE, dumplicate the connection __number__ times.\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        # simplified vertion - ignore duplications\n",
    "        G.add_nodes_from(vertices)\n",
    "        edge_list = []\n",
    "        for i,edge in enumerate(conn):\n",
    "            edge_list.append((vertices[i], vertices[i+1], edge[0])) # ignore edge[1] - determinism flag. edge[0] is attributes.\n",
    "\n",
    "        # more complex vertion - duplications\n",
    "        # create a recursive function that adds the vertices and edges, \n",
    "        # that calls itself by the number of duplications on each level.\n",
    "        G.add_edges_from(edge_list)\n",
    "        return G\n",
    "\n",
    "    def patterns(self, g, *graphs):\n",
    "        patterns = list(graphs)\n",
    "        patterns.insert(0,g)\n",
    "        # unite all the patterns into a single graph\n",
    "        G = nx.DiGraph()\n",
    "\n",
    "        combined_attributes = dict() # dict of dicts (node_name -> attribute -> value)\n",
    "        new_nodes = []\n",
    "        new_edges = []\n",
    "        for graph in patterns:\n",
    "            for node in graph.nodes:\n",
    "                combined_attributes[node] = combined_attributes[node] | graph.nodes.data()[node]\n",
    "                new_nodes.append(node) #unite the dicts for each\n",
    "            for edge in graph.edges:\n",
    "                combined_attributes[edge[0]+\",\"+edge[1]] = combined_attributes[edge[0]+\",\"+edge[1]] | graph.edges[edge[0],edge[1]]\n",
    "                new_edges.append(edge)\n",
    "\n",
    "        G.add_nodes_from([(node, combined_attributes[node]) for node in new_nodes])\n",
    "        G.add_edges_from([(node1, node2, combined_attributes[node1+\",\"+node2]) for (node1,node2) in new_edges])\n",
    "        return G"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type and constant value checking\n",
    "The transformer is designed to collect the node type and constant node value constraints, such that they are added to the 'condition' parameter to be checked later.\n",
    "\n",
    "This transformer works on a copy of the tree to keep it intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class collectTypeConstraints(Transformer):\n",
    "    def attribute(self, attr_name, type, value):\n",
    "        # return a mapping from attr_name - > required type and value\n",
    "        pass\n",
    "\n",
    "    def attributes(self, *attributes):\n",
    "        # return a packed list of the attribute mappings.\n",
    "        pass\n",
    "\n",
    "    def vertex(self, name, indices_list, attributes_list):\n",
    "        # same as lhsTransformer\n",
    "        pass\n",
    "\n",
    "    def pattern(self, vertex, *connections_to_vertex):\n",
    "        # return arguments\n",
    "        pass\n",
    "\n",
    "    def patterns(self, *patterns):\n",
    "        # unpack lists of vertices and connections.\n",
    "        def typeCondition(Match):\n",
    "            # for every vertex in vertex list:\n",
    "                # create full_vertex_name by the attached indices list\n",
    "                # for every attr, type, name required for the vertex:\n",
    "                    # constructor = getName(type) - get the constructor for the type\n",
    "                    # 1) check that the required type and value match together.\n",
    "                    # try:\n",
    "                    #     instance = constructor(value)\n",
    "                    # Except:\n",
    "                        # flag = False: value does not match the type.\n",
    "\n",
    "                    # 2) check that the value constraint holds\n",
    "                    # if getattr(instance, __eq__) == None:\n",
    "                        # flag = False. the type must implement __eq__\n",
    "                    # if not (instance == match[full_vertex_name][attr])\n",
    "\n",
    "                    # no need to check the type constraint(?), if the value fits. (python)\n",
    "\n",
    "            # TODO: perform the same iterations in the connections list.\n",
    "\n",
    "            #return flag and condition(Match)\n",
    "            pass\n",
    "\n",
    "        return typeCondition #sent as a module output and replaces condition.\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lhs_to_graph(lhs):\n",
    "    tree = lhs_parser.parse(lhs)\n",
    "    final_g = lhsTransformer().transform(tree) #networkx graph\n",
    "    return final_g"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grammar Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree = lhs_parser.parse(\"aaaaa->b\") #.pretty(indent_str = \" $ \")\n",
    "# assert(tree != None)\n",
    "# assert(tree)\n",
    "# assert(tree == \n",
    "#     Tree(Token('RULE', 'patterns'),[\n",
    "#       Tree(Token('RULE', 'pattern'),[\n",
    "#         Tree(Token('RULE', 'vertex'),[\n",
    "#           Tree(Token('RULE', 'vertex'),[]),\n",
    "#           None\n",
    "#         ])\n",
    "#       ])\n",
    "#     ]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph(nodes, edges):\n",
    "    g = nx.DiGraph()\n",
    "    g.add_nodes_from(nodes)\n",
    "    g.add_edges_from(edges)\n",
    "    return g\n",
    "\n",
    "def graphs_equal(graph1, graph2):  \n",
    "    return (\n",
    "        graph1.adj == graph2.adj\n",
    "        and graph1.nodes == graph2.nodes\n",
    "        and graph1.graph == graph2.graph\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "VisitError",
     "evalue": "Error trying to process rule \"NAMED_VERTEX\":\n\nstring index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/lark/visitors.py:137\u001b[0m, in \u001b[0;36mTransformer._call_userfunc_token\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mreturn\u001b[39;00m f(token)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m GrammarError:\n",
      "Cell \u001b[0;32mIn[57], line 50\u001b[0m, in \u001b[0;36mlhsTransformer.NAMED_VERTEX\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mNAMED_VERTEX\u001b[39m(\u001b[39mself\u001b[39m, name):\n\u001b[1;32m     49\u001b[0m     \u001b[39m# return the main name of the vertex, and an empty list.\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m     \u001b[39mreturn\u001b[39;00m (name[\u001b[39m1\u001b[39;49m], [])\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mVisitError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m graph1 \u001b[39m=\u001b[39m lhs_to_graph(\u001b[39m\"\u001b[39;49m\u001b[39ma->b\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m expected \u001b[39m=\u001b[39m create_graph([\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m],[(\u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m)])\n\u001b[1;32m      3\u001b[0m \u001b[39massert\u001b[39;00m graphs_equal(graph1, expected)\n",
      "Cell \u001b[0;32mIn[47], line 4\u001b[0m, in \u001b[0;36mlhs_to_graph\u001b[0;34m(lhs)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlhs_to_graph\u001b[39m(lhs):\n\u001b[1;32m      3\u001b[0m     tree \u001b[39m=\u001b[39m lhs_parser\u001b[39m.\u001b[39mparse(lhs)\n\u001b[0;32m----> 4\u001b[0m     final_g \u001b[39m=\u001b[39m lhsTransformer()\u001b[39m.\u001b[39;49mtransform(tree) \u001b[39m#networkx graph\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39mreturn\u001b[39;00m final_g\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/lark/visitors.py:161\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, tree)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, tree: Tree[_Leaf_T]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m _Return_T:\n\u001b[1;32m    160\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTransform the given tree, and return the final result\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 161\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform_tree(tree)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/lark/visitors.py:156\u001b[0m, in \u001b[0;36mTransformer._transform_tree\u001b[0;34m(self, tree)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform_tree\u001b[39m(\u001b[39mself\u001b[39m, tree):\n\u001b[0;32m--> 156\u001b[0m     children \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform_children(tree\u001b[39m.\u001b[39;49mchildren))\n\u001b[1;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_userfunc(tree, children)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/lark/visitors.py:146\u001b[0m, in \u001b[0;36mTransformer._transform_children\u001b[0;34m(self, children)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m children:\n\u001b[1;32m    145\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(c, Tree):\n\u001b[0;32m--> 146\u001b[0m         res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform_tree(c)\n\u001b[1;32m    147\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__visit_tokens__ \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(c, Token):\n\u001b[1;32m    148\u001b[0m         res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_userfunc_token(c)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/lark/visitors.py:156\u001b[0m, in \u001b[0;36mTransformer._transform_tree\u001b[0;34m(self, tree)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform_tree\u001b[39m(\u001b[39mself\u001b[39m, tree):\n\u001b[0;32m--> 156\u001b[0m     children \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform_children(tree\u001b[39m.\u001b[39;49mchildren))\n\u001b[1;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_userfunc(tree, children)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/lark/visitors.py:146\u001b[0m, in \u001b[0;36mTransformer._transform_children\u001b[0;34m(self, children)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m children:\n\u001b[1;32m    145\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(c, Tree):\n\u001b[0;32m--> 146\u001b[0m         res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform_tree(c)\n\u001b[1;32m    147\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__visit_tokens__ \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(c, Token):\n\u001b[1;32m    148\u001b[0m         res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_userfunc_token(c)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/lark/visitors.py:156\u001b[0m, in \u001b[0;36mTransformer._transform_tree\u001b[0;34m(self, tree)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_transform_tree\u001b[39m(\u001b[39mself\u001b[39m, tree):\n\u001b[0;32m--> 156\u001b[0m     children \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform_children(tree\u001b[39m.\u001b[39;49mchildren))\n\u001b[1;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_userfunc(tree, children)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/lark/visitors.py:148\u001b[0m, in \u001b[0;36mTransformer._transform_children\u001b[0;34m(self, children)\u001b[0m\n\u001b[1;32m    146\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform_tree(c)\n\u001b[1;32m    147\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__visit_tokens__ \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(c, Token):\n\u001b[0;32m--> 148\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_userfunc_token(c)\n\u001b[1;32m    149\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m     res \u001b[39m=\u001b[39m c\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/lark/visitors.py:141\u001b[0m, in \u001b[0;36mTransformer._call_userfunc_token\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mraise\u001b[39;00m VisitError(token\u001b[39m.\u001b[39mtype, token, e)\n",
      "\u001b[0;31mVisitError\u001b[0m: Error trying to process rule \"NAMED_VERTEX\":\n\nstring index out of range"
     ]
    }
   ],
   "source": [
    "graph1 = lhs_to_graph(\"a->b\")\n",
    "expected = create_graph([\"a\",\"b\"],[(\"a\",\"b\")])\n",
    "assert graphs_equal(graph1, expected)\n",
    "\n",
    "# pattern1 = \"a[x:number]->b\"\n",
    "# expected = create_graph([\"a\",\"b\"],[(\"a\",\"b\")])\n",
    "# assert nx.graphs_equal(pattern1, expected)\n",
    "\n",
    "# pattern1 = \"a->b\"\n",
    "# expected = create_graph([\"a\",\"b\"],[(\"a\",\"b\")])\n",
    "# assert nx.graphs_equal(pattern1, expected)\n",
    "\n",
    "# pattern1 = \"a->b\"\n",
    "# expected = create_graph([\"a\",\"b\"],[(\"a\",\"b\")])\n",
    "# assert nx.graphs_equal(pattern1, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_syntax =  \"\"\"\n",
    "a -> b\n",
    "\n",
    "a -[x:int = ...]-> b\n",
    "\n",
    "a -> b[x:int = ...]\n",
    "\n",
    "a -> b -6+[weight:int]-> c -> d[value:int]\n",
    "d<0> -> e\n",
    "d<5> -> e\n",
    "\n",
    "b -+-> d[value:int]\n",
    "d<0> -7-> e\n",
    "e<0,5> -> _\n",
    "\n",
    "b[ \\\n",
    "value: str = \\\"hello\\\", \\\n",
    "id: int \\\n",
    "]\n",
    "\n",
    "b -[\n",
    "...\n",
    "]-> c \n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
